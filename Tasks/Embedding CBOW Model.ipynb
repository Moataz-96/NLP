{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2b3e01b0170>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "from torch import nn,optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerToTensor():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self,x):\n",
    "        return torch.tensor(x,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusDataset(Dataset):\n",
    "    def __init__(self,corpus,context_size = 2,transform = None):\n",
    "        self.transform = transform\n",
    "        self.context_size = context_size\n",
    "        self.tokens = self.tokenize_corpus(corpus)\n",
    "        self.vocabulary = self.get_vocabulary(self.tokens)\n",
    "        self.word2idx = self.word2idx(self.vocabulary)\n",
    "        self.idx2word = self.idx2word(self.vocabulary)     \n",
    "        self.data = self.context_vector(self.tokens)\n",
    "        self.dataset = self.idx_pair(self.data)\n",
    "\n",
    "            \n",
    "    def get_vocabulary(self,tokens):\n",
    "        vocabulary = []\n",
    "        for sentence in tokens:\n",
    "            for token in sentence:\n",
    "                if token not in vocabulary:\n",
    "                    vocabulary.append(token)\n",
    "        return vocabulary\n",
    "    \n",
    "    def word2idx(self,vocabulary):\n",
    "        return {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "    \n",
    "    def idx2word(self,vocabulary):\n",
    "        return {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "    \n",
    "    def tokenize_corpus(self,corpus):\n",
    "        tokens = [x.split() for x in corpus]\n",
    "        return tokens\n",
    "    \n",
    "    def context_vector(self,tokens):\n",
    "        data = []\n",
    "        for sentence in tokens:\n",
    "            for i in range(0,len(sentence)): #min len(sentence) = 2\n",
    "                context = []\n",
    "                for j in reversed(range(1,self.context_size+1)):\n",
    "                    if((i-j) >= 0): #if sentence[i-j] != null\n",
    "                        context.append(sentence[i-j])\n",
    "                for j in range(1,self.context_size+1):\n",
    "                    if((i+j) < len(sentence)): #if sentence[i+j] != null\n",
    "                        context.append(sentence[i+j])\n",
    "                target = sentence[i]\n",
    "                data.append((context, target))\n",
    "        return data\n",
    "    \n",
    "    def idx_pair(self,data):\n",
    "        dataset = []\n",
    "        for input_sent,target_word in data: \n",
    "            dataset.append([[self.word2idx[input_word] for input_word in input_sent],self.word2idx[target_word]])\n",
    "        return dataset\n",
    "    \n",
    "    def show_dataset(self):\n",
    "        for input_word,output_word in self.dataset:\n",
    "            print(self.idx2word[input_word],self.idx2word[output_word])\n",
    "            \n",
    "    def __getitem__(self,idx):\n",
    "        x,y = self.dataset[idx]\n",
    "        if(self.transform):\n",
    "           #x = torch.tensor(x,dtype=torch.long)\n",
    "            x = self.transform(x)\n",
    "            y = self.transform([y])\n",
    "        return x,y \n",
    "        #it must be returned like to to be casting as set for dataLoader\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "X = list of indices input from vocab [0 ,2 ]\n",
    "V = sum(W1[X])/len(X) = sum(W1[0],W1[3],W1[2])/3\n",
    "out = dot(V,W2)\n",
    "out = softmax(out)\n",
    "W1.shape = (VxD)\n",
    "W2.shape = (DxV)\n",
    "V = (dx1)\n",
    "\n",
    "'''\n",
    "class word2vec(nn.Module):\n",
    "    def __init__(self,vocab_size,word_dims): #VxD\n",
    "        super(word2vec,self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size,word_dims)\n",
    "        self.linear = nn.Linear(word_dims,vocab_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        W1 = torch.relu(self.embed(x))\n",
    "        V = W1.sum(dim=0).view(1,-1)/len(x)\n",
    "        print(V)\n",
    "        W2 = self.linear(V)\n",
    "        out = torch.log_softmax(W2,dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus2 = [\"\"\"We are about to study the idea of a computational process.\n",
    "She me are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\"]\n",
    "data_set = CorpusDataset(corpus2,context_size = 2,transform = TransformerToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.7041, 0.4893, 0.0271, 0.8223, 0.0000, 0.6789, 1.8353,\n",
      "         0.7729]], grad_fn=<DivBackward0>)\n",
      "input =  tensor([1, 2])  \n",
      "output =  tensor([[-2.3820, -1.9742, -1.8417, -1.6153, -1.5184, -2.2069, -2.4976]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "y =  tensor([0])\n"
     ]
    }
   ],
   "source": [
    "#testing my code \n",
    "vocab_size = len(data_set.vocabulary)\n",
    "word_embed_dim = 10\n",
    "model = word2vec(vocab_size,word_embed_dim)\n",
    "yhat = model(data_set[0][0])\n",
    "print(\"input = \",data_set[0][0], \" \\noutput = \",yhat)\n",
    "print(\"y = \",data_set[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,criterion,optimizer,train_loader,iter=100):\n",
    "    for epoch in range(iter):\n",
    "        for x,y in train_loader:\n",
    "            yhat = model(x)\n",
    "            loss = criterion(yhat,y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if(epoch%100 == 0):\n",
    "            print(\"Epoch #\",epoch , \" Loss = \",loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus1 = [\n",
    "    'he is a king',\n",
    "    'she is a queen',\n",
    "    'he is a man',\n",
    "    'she is a woman',\n",
    "    'warsaw is poland capital',\n",
    "    'berlin is germany capital',\n",
    "    'paris is france capital',\n",
    "]\n",
    "corpus2 = [\"\"\"We are about to study the idea of a computational process.\n",
    "She me are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\"]\n",
    "\n",
    "corpus3 = [\"The earth revolves around the sun. The moon revolves around the earth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 0  Loss =  2.2688112258911133\n",
      "Epoch # 100  Loss =  2.074704170227051\n",
      "Epoch # 200  Loss =  1.9486188888549805\n",
      "Epoch # 300  Loss =  1.8596223592758179\n",
      "Epoch # 400  Loss =  1.7910146713256836\n",
      "Epoch # 500  Loss =  1.73349130153656\n",
      "Epoch # 600  Loss =  1.681817650794983\n",
      "Epoch # 700  Loss =  1.6330299377441406\n",
      "Epoch # 800  Loss =  1.5854477882385254\n",
      "Epoch # 900  Loss =  1.538126826286316\n",
      "Epoch # 1000  Loss =  1.4905660152435303\n",
      "Epoch # 1100  Loss =  1.4425263404846191\n",
      "Epoch # 1200  Loss =  1.3937621116638184\n",
      "Epoch # 1300  Loss =  1.3443530797958374\n",
      "Epoch # 1400  Loss =  1.2945390939712524\n",
      "Epoch # 1500  Loss =  1.2446238994598389\n",
      "Epoch # 1600  Loss =  1.194955587387085\n",
      "Epoch # 1700  Loss =  1.145906686782837\n",
      "Epoch # 1800  Loss =  1.0978474617004395\n",
      "Epoch # 1900  Loss =  1.0511244535446167\n",
      "Epoch # 2000  Loss =  1.0060362815856934\n",
      "Epoch # 2100  Loss =  0.9628233909606934\n",
      "Epoch # 2200  Loss =  0.9216533303260803\n",
      "Epoch # 2300  Loss =  0.8832783699035645\n",
      "Epoch # 2400  Loss =  0.8483127951622009\n",
      "Epoch # 2500  Loss =  0.8163626194000244\n",
      "Epoch # 2600  Loss =  0.7867724895477295\n",
      "Epoch # 2700  Loss =  0.7591265439987183\n",
      "Epoch # 2800  Loss =  0.7331610918045044\n",
      "Epoch # 2900  Loss =  0.7086565494537354\n",
      "Epoch # 3000  Loss =  0.6853161454200745\n",
      "Epoch # 3100  Loss =  0.6630712747573853\n",
      "Epoch # 3200  Loss =  0.6418524980545044\n",
      "Epoch # 3300  Loss =  0.621565580368042\n",
      "Epoch # 3400  Loss =  0.6021217703819275\n",
      "Epoch # 3500  Loss =  0.583442747592926\n",
      "Epoch # 3600  Loss =  0.565450131893158\n",
      "Epoch # 3700  Loss =  0.5480838418006897\n",
      "Epoch # 3800  Loss =  0.5312796235084534\n",
      "Epoch # 3900  Loss =  0.5150023698806763\n",
      "Epoch # 4000  Loss =  0.49920526146888733\n",
      "Epoch # 4100  Loss =  0.4838637113571167\n",
      "Epoch # 4200  Loss =  0.4689517319202423\n",
      "Epoch # 4300  Loss =  0.45445168018341064\n",
      "Epoch # 4400  Loss =  0.4403459429740906\n",
      "Epoch # 4500  Loss =  0.42662110924720764\n",
      "Epoch # 4600  Loss =  0.4132687747478485\n",
      "Epoch # 4700  Loss =  0.4002896547317505\n",
      "Epoch # 4800  Loss =  0.38767191767692566\n",
      "Epoch # 4900  Loss =  0.3754160702228546\n",
      "Epoch # 5000  Loss =  0.3635145127773285\n",
      "Epoch # 5100  Loss =  0.35196879506111145\n",
      "Epoch # 5200  Loss =  0.34077510237693787\n",
      "Epoch # 5300  Loss =  0.32992464303970337\n",
      "Epoch # 5400  Loss =  0.31941813230514526\n",
      "Epoch # 5500  Loss =  0.30925479531288147\n",
      "Epoch # 5600  Loss =  0.2994348704814911\n",
      "Epoch # 5700  Loss =  0.28993725776672363\n",
      "Epoch # 5800  Loss =  0.2807689309120178\n",
      "Epoch # 5900  Loss =  0.2719276249408722\n",
      "Epoch # 6000  Loss =  0.2634000778198242\n",
      "Epoch # 6100  Loss =  0.25517600774765015\n",
      "Epoch # 6200  Loss =  0.24725067615509033\n",
      "Epoch # 6300  Loss =  0.23961913585662842\n",
      "Epoch # 6400  Loss =  0.23227399587631226\n",
      "Epoch # 6500  Loss =  0.22520659863948822\n",
      "Epoch # 6600  Loss =  0.21840517222881317\n",
      "Epoch # 6700  Loss =  0.21186739206314087\n",
      "Epoch # 6800  Loss =  0.2055816799402237\n",
      "Epoch # 6900  Loss =  0.1995369791984558\n",
      "Epoch # 7000  Loss =  0.19372408092021942\n",
      "Epoch # 7100  Loss =  0.18813884258270264\n",
      "Epoch # 7200  Loss =  0.1827627718448639\n",
      "Epoch # 7300  Loss =  0.17760799825191498\n",
      "Epoch # 7400  Loss =  0.17265021800994873\n",
      "Epoch # 7500  Loss =  0.16788485646247864\n",
      "Epoch # 7600  Loss =  0.16330993175506592\n",
      "Epoch # 7700  Loss =  0.15890945494174957\n",
      "Epoch # 7800  Loss =  0.15467509627342224\n",
      "Epoch # 7900  Loss =  0.15059882402420044\n",
      "Epoch # 8000  Loss =  0.14668779075145721\n",
      "Epoch # 8100  Loss =  0.14292579889297485\n",
      "Epoch # 8200  Loss =  0.13930602371692657\n",
      "Epoch # 8300  Loss =  0.1358119249343872\n",
      "Epoch # 8400  Loss =  0.13243988156318665\n",
      "Epoch # 8500  Loss =  0.1291923075914383\n",
      "Epoch # 8600  Loss =  0.12606710195541382\n",
      "Epoch # 8700  Loss =  0.12305058538913727\n",
      "Epoch # 8800  Loss =  0.12014905363321304\n",
      "Epoch # 8900  Loss =  0.11735251545906067\n",
      "Epoch # 9000  Loss =  0.11465645581483841\n",
      "Epoch # 9100  Loss =  0.11206033080816269\n",
      "Epoch # 9200  Loss =  0.1095552071928978\n",
      "Epoch # 9300  Loss =  0.10713814198970795\n",
      "Epoch # 9400  Loss =  0.1048077717423439\n",
      "Epoch # 9500  Loss =  0.10255683958530426\n",
      "Epoch # 9600  Loss =  0.10038213431835175\n",
      "Epoch # 9700  Loss =  0.09828135371208191\n",
      "Epoch # 9800  Loss =  0.0962529182434082\n",
      "Epoch # 9900  Loss =  0.09429065883159637\n"
     ]
    }
   ],
   "source": [
    "data_set = CorpusDataset(corpus3,context_size = 2,transform = TransformerToTensor())\n",
    "#train_loader = DataLoader(dataset = data_set,batch_size=32)\n",
    "#hint: i cannot use DataLoader because input not the same shape\n",
    "#if you want to use DataLoader you have to make input fixed size it's the only options and it's not hard \n",
    "vocab_size = len(data_set.vocabulary)\n",
    "word_embed_dim = 10\n",
    "model = word2vec(vocab_size,word_embed_dim)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)\n",
    "criterion = nn.NLLLoss()\n",
    "train_model(model,criterion,optimizer,data_set,iter=10000)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You can choose your embedded matrix as W1 or W2 or (W+W`)/2\n",
    "but i will choose W1 here\n",
    "note that :\n",
    "W1 = word2vec.embed\n",
    "W2 = word2vec.linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 10])\n",
      "torch.Size([7, 10])\n"
     ]
    }
   ],
   "source": [
    "#print(model.state_dict()['embed.weight'].size())\n",
    "W1 = model.state_dict()['embed.weight'].clone().detach()\n",
    "W2 = model.state_dict()['linear.weight'].clone().detach()\n",
    "print(W1.shape)\n",
    "print(W2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_words(word,top_k,Weights):\n",
    "    idx = torch.tensor(data_set.word2idx[word])\n",
    "    #W1(VxD) dot word_vec(Dx1) = Vx1 \n",
    "    word_vec = torch.tensor(Weights[idx]).view(-1,1)\n",
    "    yhat = torch.mm(Weights,word_vec)\n",
    "    top_k = 4\n",
    "    values , most_similar_idx =  torch.topk(yhat, k=top_k, dim=0)\n",
    "    for i in range(top_k):\n",
    "        most_similar_word = data_set.idx2word[most_similar_idx[i].item()]\n",
    "        if(most_similar_word == word):\n",
    "            continue\n",
    "        print(most_similar_word,values[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With weights W1 (nn.Embedded)\n",
      "\n",
      "The 10.775361061096191\n",
      "revolves 7.231320381164551\n",
      "sun. 4.813801288604736\n",
      "\n",
      "With Weights W2 (nn.Linear)\n",
      "\n",
      "The 8.588621139526367\n",
      "revolves 3.4847989082336426\n",
      "sun. -1.274202585220337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muata\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "word = \"around\"\n",
    "print(\"With weights W1 (nn.Embedded)\\n\")\n",
    "get_similar_words(word,top_k=3,Weights=W1)\n",
    "print(\"\\nWith Weights W2 (nn.Linear)\\n\")\n",
    "get_similar_words(word,top_k=3,Weights=W2)\n",
    "\n",
    "#W1 gives best results "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "why there is two matrices W1 , W2?\n",
    "answer:\n",
    "https://stackoverflow.com/questions/29381505/why-does-word2vec-use-2-representations-for-each-word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
