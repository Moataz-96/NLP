{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Problem:\n",
    "Problem:input sentence / output English/Spanish\n",
    "Model:Bag Of Vectors - Bag Of Words (BOW) #hint:not cbow\n",
    "Details:\n",
    "This model not using any pretrained vectors (word2vec nor glove)\n",
    "It starts from very scratch dataset with one hot encoding\n",
    "#ref: https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html#sphx-glr-beginner-nlp-deep-learning-tutorial-py\n",
    "\n",
    "\n",
    "<img src=\"BagOfWords_Model.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1ef5c180170>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn,optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CudaDeviceProperties(name='GeForce RTX 2060', major=7, minor=5, total_memory=6144MB, multi_processor_count=30)\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "print(torch.cuda.get_device_properties(device))\n",
    "print(device)\n",
    "#however iam using cpu in that model it's not big dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
    "        (\"Give it to me\".split(), \"ENGLISH\"),\n",
    "        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
    "        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n",
    "\n",
    "test_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
    "             (\"it is lost on me\".split(), \"ENGLISH\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'me': 0, 'gusta': 1, 'comer': 2, 'en': 3, 'la': 4, 'cafeteria': 5, 'Give': 6, 'it': 7, 'to': 8, 'No': 9, 'creo': 10, 'que': 11, 'sea': 12, 'una': 13, 'buena': 14, 'idea': 15, 'is': 16, 'not': 17, 'a': 18, 'good': 19, 'get': 20, 'lost': 21, 'at': 22, 'Yo': 23, 'si': 24, 'on': 25}\n"
     ]
    }
   ],
   "source": [
    "#word to index each word in data\n",
    "word_to_idx = {}\n",
    "for sentance,_ in train_data+test_data:\n",
    "    for word in sentance:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "print(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_idx)\n",
    "num_of_labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWClassifier(nn.Module):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super(BoWClassifier,self).__init__()\n",
    "        self.linear = nn.Linear(input_size,output_size)\n",
    "    def forward(self,bow_vector):\n",
    "        #output = torch.log_softmax(self.linear(bow_vector),dim=1)\n",
    "        output = F.log_softmax(self.linear(bow_vector), dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self,data,word_to_idx,label_to_ix):\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        self.word_to_idx = word_to_idx\n",
    "        for i,(sentance,label) in enumerate(data):\n",
    "            self.x.append(sentance)\n",
    "            self.y.append(label_to_ix[label])\n",
    "        \n",
    "    def __getitem__(self,idx):       \n",
    "        target = torch.tensor(self.y[idx]).view(-1)\n",
    "        sentance = self.x[idx]\n",
    "        one_hot_vec = torch.zeros(len(sentance),len(self.word_to_idx))\n",
    "        for i,word in enumerate(sentance):\n",
    "            one_hot_vec[i,self.word_to_idx[word]] = 1\n",
    "        bag_vector = one_hot_vec.sum(0)\n",
    "        '''\n",
    "        #you can replace last 4 line with next 3 lines but however uncomment part is general. \n",
    "        one_hot_vec = torch.zeros(len(self.word_to_idx))\n",
    "        for word in sentance:\n",
    "            one_hot_vec[self.word_to_idx[word]] += 1\n",
    "        '''\n",
    "        #take average of one_hot_vec for sentence\n",
    "        \n",
    "        return bag_vector.view(1, -1),target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BoWClassifier(vocab_size,num_of_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1011, -0.0866, -0.0380,  0.0921, -0.1846,  0.1176, -0.0403,  0.0998,\n",
      "          0.0273, -0.0240,  0.0544,  0.0097,  0.0716, -0.0764, -0.0143, -0.0177,\n",
      "          0.0284, -0.0008,  0.1714,  0.0610, -0.0730, -0.1184, -0.0329, -0.0846,\n",
      "         -0.0628,  0.0094],\n",
      "        [ 0.1169,  0.1066, -0.1917,  0.1216,  0.0548,  0.1860,  0.1294, -0.1787,\n",
      "         -0.1865, -0.0946,  0.1722, -0.0327,  0.0839, -0.0911,  0.1924, -0.0830,\n",
      "          0.1471,  0.0023, -0.1033,  0.1008, -0.1041,  0.0577, -0.0566, -0.0215,\n",
      "         -0.1885, -0.0935]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1064, -0.0477], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note that the input to NLLLoss is a vector of log probabilities, and a target label. It doesnâ€™t compute the log probabilities for us. This is why the last layer of our network is log softmax. The loss function nn.CrossEntropyLoss() is the same as NLLLoss(), except it does the log softmax for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_ix = {\"SPANISH\": 0, \"ENGLISH\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset(train_data,word_to_idx,label_to_ix)\n",
    "test_dataset = dataset(test_data,word_to_idx,label_to_ix)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.1)\n",
    "criterion = nn.NLLLoss()\n",
    "def train_model(iter):\n",
    "    for epoch in range(iter):\n",
    "        for x,y in train_dataset:\n",
    "            yhat = model(x)\n",
    "            loss = criterion(yhat,y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6250, -0.7662]])\n",
      "tensor([[-0.5870, -0.8119]])\n",
      "tensor([0.0544, 0.1722], grad_fn=<SelectBackward>)\n",
      "tensor([[-0.1210, -2.1721]])\n",
      "tensor([[-2.7767, -0.0643]])\n",
      "tensor([ 0.5004, -0.2738], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "#in testing phase you must use torch.no_grad() or replace x with x.detach()\n",
    "#because without doing it , gradient will track this part of code if training again it will take more time\n",
    "#or maybe got wrong values\n",
    "with torch.no_grad():\n",
    "    for x,y in test_dataset:\n",
    "        predict_before_train = model(x)\n",
    "        print(predict_before_train)\n",
    "\n",
    "print(next(model.parameters())[:, word_to_idx[\"creo\"]])\n",
    "train_model(100)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x,y in test_dataset:\n",
    "        predict_after_train = model(x)\n",
    "        print(predict_after_train)\n",
    "\n",
    "print(next(model.parameters())[:, word_to_idx[\"creo\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word ' que ' predicted as : SPANISH\n"
     ]
    }
   ],
   "source": [
    "#predict word \"Give\"\n",
    "prediction_word = \"que\" #que / good / Give\n",
    "tens =  next(model.parameters())[:, word_to_idx[prediction_word]]\n",
    "value,index = tens.max(0)\n",
    "print(\"word '\",prediction_word,\"' predicted as :\", list(label_to_ix.keys())[index])  \n",
    "#English "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
