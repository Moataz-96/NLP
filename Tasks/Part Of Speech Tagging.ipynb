{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d538131170>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "from torch import nn,optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerToTensor():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self,x):\n",
    "        return torch.tensor(x,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self,corpus,transform=None):\n",
    "        self.transform = transform\n",
    "        self.tokens = self.tokenize_corpus(corpus)\n",
    "        self.vocabulary,self.tags = self.get_vocabulary(self.tokens)\n",
    "        self.word2idx = self.get_word2idx(self.vocabulary)\n",
    "        self.idx2word = self.get_idx2word(self.vocabulary)\n",
    "        self.tag2idx = self.get_word2idx(self.tags)\n",
    "        self.idx2tag = self.get_idx2word(self.tags) \n",
    "        \n",
    "        self.dataset = self.idx_pair(self.tokens)\n",
    "    \n",
    "    def tokenize_corpus(self,corpus):\n",
    "        tokens = [(x.split(),y) for x,y in corpus]\n",
    "        return tokens\n",
    "    \n",
    "    def get_vocabulary(self,tokens):\n",
    "        vocabulary = []\n",
    "        tags = []\n",
    "        for sentence,tags_vocab in tokens:\n",
    "            for token in sentence:\n",
    "                if token not in vocabulary:\n",
    "                    vocabulary.append(token)\n",
    "            for tag in tags_vocab:\n",
    "                if tag not in tags:\n",
    "                    tags.append(tag)\n",
    "        return vocabulary,tags\n",
    "    \n",
    "    def get_word2idx(self,vocabulary):\n",
    "        return {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "    \n",
    "    def get_idx2word(self,vocabulary):\n",
    "        return {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "    \n",
    "    \n",
    "    def idx_pair(self,data):\n",
    "        dataset = []\n",
    "        for input_sent,tag_sent in data: \n",
    "            dataset.append(([self.word2idx[word] for word in input_sent],[self.tag2idx[tag] for tag in tag_sent]))\n",
    "        return dataset\n",
    "    \n",
    "    def show_dataset(self):\n",
    "        for input_word,output_word in self.dataset:\n",
    "            print(self.idx2word[input_word],self.idx2word[output_word])\n",
    "            \n",
    "    def __getitem__(self,idx):\n",
    "        x,y = self.dataset[idx]\n",
    "        if(self.transform):\n",
    "            x = self.transform(x)\n",
    "            y = self.transform(y)\n",
    "        return x,y \n",
    "        #it must be returned like to to be casting as set for dataLoader\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([5, 6, 7, 8]), tensor([1, 2, 0, 1]))\n"
     ]
    }
   ],
   "source": [
    "training_data = [\n",
    "    (\"The dog ate the apple\", [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\", [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "pos_dataset = dataset(training_data,transform=TransformerToTensor())\n",
    "print(pos_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self,embedd_dim,hidden_dim,vocab_size,tagset_size):\n",
    "        super(LSTMTagger,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(vocab_size,embedd_dim)\n",
    "        self.lstm = nn.LSTM(embedd_dim,hidden_dim,bidirectional=False)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim,tagset_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        inputs_seq = self.embedding(x)\n",
    "        a_out , _ = self.lstm(inputs_seq.view(len(x), 1, -1)) \n",
    "        #this is not sequance model so we don't need hidden_states a,c to be accamulated\n",
    "        output_tags = self.hidden2tag(a_out.view(len(x), -1))\n",
    "        output_scores = F.log_softmax(output_tags,dim = 1)\n",
    "        return output_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(pos_dataset.word2idx)\n",
    "tagset_size = len(pos_dataset.tag2idx)\n",
    "embedding_dims = 6\n",
    "hidden_dim = 6\n",
    "model = LSTMTagger(embedding_dims,hidden_dim,vocab_size,tagset_size)\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.1)\n",
    "criterion = nn.NLLLoss()\n",
    "def train_model(model,optimizer,criterion,dataset,iter = 10):\n",
    "    for i in range(iter):\n",
    "        for x,y in dataset:\n",
    "            yhat = model(x)\n",
    "            loss = criterion(yhat,y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = train_model(model,optimizer,criterion,pos_dataset,iter = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input =  tensor([0, 1, 2, 3])  output =  tensor([0, 1, 2, 0])\n",
      "word : Everybody  Tag : NN\n",
      "word : ate  Tag : V\n",
      "word : the  Tag : DET\n",
      "word : book  Tag : NN\n"
     ]
    }
   ],
   "source": [
    "# See what the scores are after training\n",
    "testing_data = [\n",
    "    (\"Everybody ate the book\", [\"NN\",\"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "test_dataset = dataset(testing_data,transform=TransformerToTensor() )\n",
    "for x,y in test_dataset:\n",
    "    print(\"input = \",x,\" output = \",y)\n",
    "with torch.no_grad():\n",
    "    for x,_ in test_dataset:\n",
    "        tag_scores = model(x)\n",
    "        max_tag_scores_values,max_tag_scores_indices = tag_scores.max(dim=1) \n",
    "        for i in range(len(x)):\n",
    "            print(\"word :\",test_dataset.idx2word[x[i].item()],\" Tag :\",test_dataset.idx2tag[max_tag_scores_indices[i].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
