{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinetalk.org/2019/02/08/text-generation-with-pytorch/\n",
    "https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html#example-an-lstm-for-part-of-speech-tagging\n",
    "https://blog.exxactcorp.com/getting-started-with-natural-language-processing-using-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x26645a01190>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "from torch import nn,optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "from argparse import Namespace\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = Namespace(\n",
    "    train_file='Oliver.txt',\n",
    "    seq_size=32,\n",
    "    batch_size=16,\n",
    "    embedding_size=64,\n",
    "    lstm_size=64,\n",
    "    gradients_norm=5,\n",
    "    initial_words=['I', 'am'],\n",
    "    predict_top_k=5,\n",
    "    checkpoint_path='checkpoint',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_file(train_file, batch_size, seq_size):\n",
    "    with open(train_file, 'r') as f:\n",
    "        text = f.read()\n",
    "    text = text.split()\n",
    "    print(\"length = \",len(text))\n",
    "    word_counts = Counter(text)\n",
    "    \n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
    "    n_vocab = len(int_to_vocab)\n",
    "\n",
    "    print('Vocabulary size', n_vocab)\n",
    "\n",
    "    int_text = [vocab_to_int[w] for w in text]\n",
    "    num_batches = int(len(int_text) / (seq_size * batch_size))\n",
    "    in_text = int_text[:num_batches * batch_size * seq_size]\n",
    "    out_text = np.zeros_like(in_text)\n",
    "    out_text[:-1] = in_text[1:]\n",
    "    out_text[-1] = in_text[0]\n",
    "    in_text = np.reshape(in_text, (batch_size, -1))\n",
    "    out_text = np.reshape(out_text, (batch_size, -1))\n",
    "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(in_text, out_text, batch_size, seq_size):\n",
    "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
    "    for i in range(0, num_batches * seq_size, seq_size):\n",
    "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length =  9873\n",
      "Vocabulary size 3187\n"
     ]
    }
   ],
   "source": [
    "int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(flags.train_file,flags.batch_size,flags.seq_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9728\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(in_text.shape[0]*in_text.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerToTensor():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self,x):\n",
    "        return torch.tensor(x,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self,corpus_file,sequance_length=32,batch_size=16,transform=None):\n",
    "        self.transform = transform\n",
    "        self.sequance_length = sequance_length\n",
    "        self.batch_size = batch_size\n",
    "        self.text = self.read_from_file(corpus_file)\n",
    "        self.tokens = self.tokenize_corpus(self.text)\n",
    "        self.vocabulary = self.get_vocabulary(self.tokens)\n",
    "        self.word2idx = self.get_word2idx(self.vocabulary)\n",
    "        self.idx2word = self.get_idx2word(self.vocabulary)\n",
    "        self.indices= self.tokens_to_indices(self.tokens)\n",
    "        self.dataset = self.idx_pair(self.tokens)\n",
    "        self.data_loader = self.batchify()\n",
    "    def read_from_file(self,file):\n",
    "        with open(file, 'r') as f:\n",
    "            text = f.read()\n",
    "        return text\n",
    "    def tokens_to_indices(self,tokens):\n",
    "        return [self.word2idx[token] for token in tokens]\n",
    "        \n",
    "    def tokenize_corpus(self,corpus):\n",
    "        tokens = corpus.split()\n",
    "        return tokens\n",
    "    \n",
    "    def get_vocabulary(self,tokens):\n",
    "        vocabulary = []\n",
    "        for word in tokens:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary.append(word)\n",
    "        return vocabulary\n",
    "    \n",
    "    def get_word2idx(self,vocabulary):\n",
    "        return {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "    \n",
    "    def get_idx2word(self,vocabulary):\n",
    "        return {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "    \n",
    "    \n",
    "    def idx_pair(self,data):\n",
    "        dataset = []\n",
    "        x_shape = len(data)//self.sequance_length\n",
    "        for i in range(0,x_shape-1): \n",
    "            x_start_idx = i*self.sequance_length\n",
    "            x_end_idx = (i+1)*self.sequance_length\n",
    "            y_start_idx = x_start_idx + 1\n",
    "            y_end_idx = x_end_idx + 1\n",
    "            x_i = [self.word2idx[word] for word in data[x_start_idx:x_end_idx]]\n",
    "            y_i = [self.word2idx[word] for word in data[y_start_idx:y_end_idx]]\n",
    "            dataset.append(([x_i,y_i]))\n",
    "        return dataset\n",
    "    \n",
    "    def show_dataset(self):\n",
    "        for input_word,output_word in self.dataset:\n",
    "            print(self.idx2word[input_word],self.idx2word[output_word])\n",
    "            \n",
    "    def batchify(self):\n",
    "        batches_length = len(self.dataset)//self.batch_size\n",
    "        data_loader = []\n",
    "        self.dataset = np.array(self.dataset)\n",
    "        for i in range(0,batches_length):\n",
    "            start_idx = i*self.batch_size\n",
    "            end_idx = (i+1)*self.batch_size\n",
    "            data_loader.append([self.dataset[start_idx:end_idx,0],self.dataset[start_idx:end_idx,1]])\n",
    "        return data_loader\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        batch_x,batch_y = self.data_loader[idx]\n",
    "        if(self.transform):\n",
    "            batch_x = self.transform(batch_x)\n",
    "            batch_y = self.transform(batch_y)\n",
    "        return batch_x,batch_y \n",
    "        #it must be returned like to to be casting as set for dataLoader\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3187\n"
     ]
    }
   ],
   "source": [
    "batch_size =307\n",
    "sequance_length = 32\n",
    "data_set = dataset(flags.train_file,sequance_length,batch_size,transform=TransformerToTensor()) \n",
    "assert data_set[0][0].shape == (batch_size,sequance_length)\n",
    "print(len(data_set.vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307\n",
      "shape of one batch : torch.Size([307, 32])\n",
      "[[   0    1    2 ...    2   25   26]\n",
      " [  27   28   29 ...   51   52   53]\n",
      " [  54   55   56 ...   79   66   80]\n",
      " ...\n",
      " [ 547  141 1949 ...  673 3159   83]\n",
      " [3160   59  103 ...  919 2503 1255]\n",
      " [  99  152 3167 ...   66 1340 2223]]\n"
     ]
    }
   ],
   "source": [
    "print(len(data_set))\n",
    "batches = data_set.data_loader\n",
    "print(\"shape of one batch :\",data_set[0][0].shape)\n",
    "for x,y in batches:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#NLLloss function require shape (batch_size,vocabulary_length=classes,sequance_length)\n",
    "for a target of (batch_size,sequance_length )\n",
    "so if y = (batch_size,sequance_length) then yhat should be = (batch_size,vocabulary_length=classes,sequance_length)\n",
    "\n",
    "then make your softmax should be         \n",
    "output_scores = F.log_softmax(dense.view(self.batch_size,-1,self.sequance_length),dim = 1)\n",
    "test your softmax but replace first log_softmax with softmax then\n",
    "print(output_scores.sum(dim=1)) #should == 1\n",
    "\n",
    "---\n",
    "#before we start\n",
    "sequance_length has no effect on accuracy fa lw d5lt el Tx = 1 zy mhtd5lha Tx=9000\n",
    "el etnen zy b3d belzbt w dah 3la kol el models Text Generation w Text Classification,...\n",
    "3shan enta mohtm b a5r out_state bs\n",
    "tp leh bst5dm el seq_len 3shan el loss time by2l wfrt computation bdl m3ml le kol word time loss\n",
    "b2et le kol batch_size*sequance_length words\n",
    "y3ne enta wfrt el time bnsbt el batch_size*sequance_length dah lw gpu\n",
    "enma lw cpu msh htfr2 el Tx 5ales\n",
    "enma lw 5let el batch_size =1 wel seq_len=1 el accuracy msh htzed ay 7aga\n",
    "el batch_size momken yzwd el acc shwya SGD a7sn mn el Vectorization fel perforamnce\n",
    "\n",
    "bs eftkr en el batch_size = 607 nfs el run time bta3 batch_size 1 3shan el gpu fel 7lten hy7slha\n",
    "access mra w7da 3shan el batch_size=607 msh kpir\n",
    "fa e3ml el batch_size = 607 3shan tshof el result a7sn\n",
    "---\n",
    "fel model dah ana 2smt el input text corpus le words trainings_ex*batch_size*sequance_length\n",
    "el size msh hyege belzbt hytb2a shwyt training example ana shelthom bs el mfrod t3ml 7sabhom\n",
    "---\n",
    "lstm = nn.LSTM(embedd_dim,hidden_dim,bidirectional=False,batch_first=False)\n",
    "#num_directions = 1 if bidrectional=False and = 2 if bidrectional=True\n",
    "#there is two inputs to lstm and two outputs\n",
    "\n",
    "inputs = x , shape = (sequance_length,batch_size,embedded_dims)\n",
    "hiddens = (a.shape==(num_layers * num_directions , batch_size, hidden_size)\n",
    "           c.shape==(num_layers * num_directions , batch_size, hidden_size))\n",
    "logits,states = lstm(inputs,hidden)\n",
    "\n",
    "logits shape = (sequance_length,batch_size,embedded_dims)\n",
    "hiddens = (a.shape==(num_layers * num_directions , batch_size,num_directions* hidden_size)\n",
    "           c.shape==(num_layers * num_directions , batch_size,num_directions* hidden_size))\n",
    "\n",
    "if batch_first = True\n",
    "then the changing will be in \n",
    "lstm = nn.LSTM(embedd_dim,hidden_dim,bidirectional=False,batch_first=True)\n",
    "inputs = x , shape = (batch_size,sequance_length,embedded_dims)\n",
    "logits shape = (batch_size,sequance_length,embedded_dims)\n",
    "\n",
    "at prediction time \n",
    "inputs = x , shape = (1,1,embedded_dims)\n",
    "logits shape = (1,1,embedded_dims)\n",
    "\n",
    "if bidirectional = True\n",
    "remember if LSTM is biderctional it computes from left to right and right from left independantly\n",
    "then concatenate a<tx> so if num_hiddens = 32 then out_hiddens = 64\n",
    "so you have to optinons to change\n",
    "\n",
    "1- shape of input to dense remain constant = 32 then you need to make num_hidden = 16\n",
    "self.lstm = nn.LSTM(embedd_dim,hidden_dim//2,bidirectional=True ,batch_first=True)\n",
    "self.dense = nn.Linear(hidden_dim,vocab_size)\n",
    "self.hiddens =  (torch.zeros(2, self.batch_size, self.hidden_dim//2).cuda(),\n",
    "                torch.zeros(2, self.batch_size, self.hidden_dim//2).cuda())\n",
    "    \n",
    "2- shape of input to dense became double of hidden states\n",
    "self.lstm = nn.LSTM(embedd_dim,hidden_dim,bidirectional=True ,batch_first=True)\n",
    "self.dense = nn.Linear(2*hidden_dim,vocab_size)\n",
    "self.hiddens =  (torch.zeros(2, self.batch_size, self.hidden_dim).cuda(),\n",
    "                torch.zeros(2, self.batch_size, self.hidden_dim).cuda())\n",
    "              \n",
    "if num_of_layers > 1\n",
    "you have two options:hint first option more computation efficient\n",
    "1-\n",
    "lstm = nn.LSTM(embedd_dim,hidden_dim,bidirectional=False,num_layers=num_of_layers,batch_first=False)\n",
    "hiddens = (a.shape==(num_layers * num_directions , batch_size,num_directions* hidden_size)\n",
    "           c.shape==(num_layers * num_directions , batch_size,num_directions* hidden_size))\n",
    "\n",
    "#DONT DONT NEXT TWO LINES \n",
    "a_out ,self.hidden = self.lstm(embedded_vector.view(self.batch_size,len(x), -1),self.hidden//self.num_of_directions)\n",
    "#DO THAT \n",
    "a_out ,self.hidden = self.lstm(embedded_vector.view(self.batch_size,len(x), -1),self.hidden//self.num_of_directions)\n",
    "\n",
    "2-\n",
    "lstm = nn.LSTM(embedd_dim,hidden_dim,bidirectional=False,num_layers=1,batch_first=False)\n",
    "hiddens1 = (a.shape==(1* num_directions , batch_size,num_directions* hidden_size)\n",
    "           c.shape==(1 * num_directions , batch_size,num_directions* hidden_size))\n",
    "hiddens2 = (a.shape==(1* num_directions , batch_size,num_directions* hidden_size)\n",
    "           c.shape==(1 * num_directions , batch_size,num_directions* hidden_size))     \n",
    "           \n",
    "a_out1 ,self.hidden1 = self.lstm(embedded_vector,self.hidden1)\n",
    "a_out2 ,self.hidden2 = self.lstm(embedded_vector,self.hidden2)\n",
    "------------------\n",
    "prediction:\n",
    "if you was doing that next line prediction time will be error and not very hard to do prediction\n",
    "a_out ,self.hidden = self.lstm(embedded_vector.view(self.batch_size,len(x), -1),self.hidden//self.num_of_directions)\n",
    "\n",
    "but if you let lstm determine the shape it will be good for prediction time so do that\n",
    "a_out ,self.hidden = self.lstm(embedded_vector,self.hidden//self.num_of_directions)\n",
    "\n",
    "tany 7aga blash fel training t3ml el \n",
    "self.hiddens =  (torch.zeros(2, self.batch_size, self.hidden_dim//2).cuda(),torch.zeros(2, self.batch_size, self.hidden_dim//2).cuda())\n",
    "tp leh ?\n",
    "3shan fel predict enta lazm t3ml el state_a,state_c = zeros awl mra fa enta kda ya ema htbwz el self.hidden el aslya w t5leha self.hidden = zeros tany b3d m3mlt el training w fel 7ala dy msh ht3rf tkml training b3d el predict\n",
    "fa el afdl enk htb3t kol mra state_a,state_c m3 el x\n",
    "fel training w fel testing time\n",
    "fel training hykono sbten w fel test awl mra hykono b zero\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerationModel(nn.Module):\n",
    "    def __init__(self,batch_size,sequance_length,embedd_dim,hidden_dim,vocab_size,bidirectional=False):\n",
    "        super(TextGenerationModel,self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_of_directions = 1\n",
    "        if(self.bidirectional):\n",
    "            self.num_of_directions = 2\n",
    "        self.batch_size = batch_size\n",
    "        self.sequance_length = sequance_length\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(vocab_size,embedd_dim)\n",
    "        self.lstm = nn.LSTM(embedd_dim,hidden_dim//self.num_of_directions,bidirectional=self.bidirectional ,batch_first=True)\n",
    "        self.dense = nn.Linear(hidden_dim,vocab_size)\n",
    "\n",
    "    def forward(self,x,prev_state):\n",
    "        embedded_vector = self.embedding(x)\n",
    "        a_out ,prev_state = self.lstm(embedded_vector,prev_state)\n",
    "        #don't specify a_out ,self.hidden = self.lstm(embedded_vector.view(self.batch_size,self.sequance_length, -1),self.hidden)\n",
    "        dense = self.dense(a_out)\n",
    "        output_scores = F.log_softmax(dense.view(embedded_vector.shape[0],-1,embedded_vector.shape[1]),dim = 1)        \n",
    "        return output_scores,prev_state\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_states(num_of_directions,batch_size,hidden_dim):\n",
    "    (torch.zeros(num_of_directions, batch_size, hidden_dim//num_of_directions).cuda(),\n",
    "     torch.zeros(num_of_directions, batch_size, hidden_dim//num_of_directions).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(init_words,num_of_words_predict,top_k=3):\n",
    "    model.eval()\n",
    "    states = init_states(num_of_directions=1,batch_size=1,hidden_dim=hidden_dim)\n",
    "    words = []\n",
    "    for word in init_words:\n",
    "        word_to_idx = torch.tensor([[data_set.word2idx[word]]])\n",
    "        out,states= model(word_to_idx.cuda(),states)\n",
    "    _, top_ix = torch.topk(out[0,:,0], k=top_k)\n",
    "    choices = top_ix.tolist()\n",
    "    choice = np.random.choice(choices[0])\n",
    "    words.append(data_set.idx2word[choice])\n",
    "    for _ in range(num_of_words_predict):\n",
    "        word_to_idx = torch.tensor([[data_set.word2idx[word]]])\n",
    "        out,states = model(word_to_idx.cuda(),states)\n",
    "        \n",
    "        _, top_ix = torch.topk(out[0,:,0], k=top_k)\n",
    "        choices = top_ix.tolist()\n",
    "        choice = np.random.choice(choices[0])\n",
    "        words.append(data_set.idx2word[choice])\n",
    "    print(' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(data_set.word2idx)\n",
    "embedding_dims = flags.embedding_size\n",
    "hidden_dim = flags.lstm_size\n",
    "model = TextGenerationModel(batch_size,sequance_length,embedding_dims,hidden_dim,vocab_size,True).cuda()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion = nn.NLLLoss()\n",
    "prev_state = init_states(num_of_directions=1,batch_size=batch_size,hidden_dim=hidden_dim)\n",
    "\n",
    "def train_model(prev_state,model,optimizer,criterion,dataset,iter = 10):\n",
    "    for i in range(iter):     \n",
    "        for x,y in dataset:\n",
    "            model.train()\n",
    "            yhat,prev_state = model(x.cuda(),prev_state)\n",
    "            loss = criterion(yhat,y.cuda())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "        print(\"loss at epoch :\",i ,\" = \",loss.item())\n",
    "        print(\"SAMPLE :\")\n",
    "        predict(\"the\".split(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch : 0  =  8.075037002563477\n",
      "SAMPLE :\n",
      "little garden-gate. engendered, perhaps fed, did Among trouble, Vilkins. coverlet death\n",
      "loss at epoch : 1  =  8.068414688110352\n",
      "SAMPLE :\n",
      "indubitably end wretched badged troublesome. this accident; floor sitting somewhat direction\n",
      "loss at epoch : 2  =  8.06159782409668\n",
      "SAMPLE :\n",
      "proceeded did only end PLACE think,' 'Just darkly offered quite be.\n",
      "loss at epoch : 3  =  8.054750442504883\n",
      "SAMPLE :\n",
      "refusal, mother, hungry, With children! Apparently dignity was beadle. human Wrapped\n",
      "loss at epoch : 4  =  8.04787826538086\n",
      "SAMPLE :\n",
      "beadle, both sickened hungry putting glanced did merit implicitly about estimate\n",
      "loss at epoch : 5  =  8.04096508026123\n",
      "SAMPLE :\n",
      "necessary fire: one, 'Oliver father, buildings washing. for, perception destitute accurate\n",
      "loss at epoch : 6  =  8.033980369567871\n",
      "SAMPLE :\n",
      "not time 'I, consolatory That chair hungry, AND deposited Ah! good,\n",
      "loss at epoch : 7  =  8.0269136428833\n",
      "SAMPLE :\n",
      "comprised dear mind opportunity nothing eyes. birthday hope body, possess Part\n",
      "loss at epoch : 8  =  8.01972770690918\n",
      "SAMPLE :\n",
      "matter culprits two from, buffeted lying jury happened TWIST'S to, interesting\n",
      "loss at epoch : 9  =  8.012399673461914\n",
      "SAMPLE :\n",
      "upon gave show order. shuddered; fire: thereon. less feel upward, balance\n",
      "loss at epoch : 10  =  8.00492000579834\n",
      "SAMPLE :\n",
      "Little. accident humility, orphan, purpose, took take even health evident sir!'\n",
      "loss at epoch : 11  =  7.997261047363281\n",
      "SAMPLE :\n",
      "could rebel interest observed firmly humility. THE TREATS eight Christian.' salutation\n",
      "loss at epoch : 12  =  7.989397048950195\n",
      "SAMPLE :\n",
      "rather was,' beadle's pick EDUCATION, stupefied voice, crouching born; seals. narrative\n",
      "loss at epoch : 13  =  7.981314182281494\n",
      "SAMPLE :\n",
      "protection appropriated tortures inheritance seem reasonably protecting men: agony Of that\n",
      "loss at epoch : 14  =  7.972977638244629\n",
      "SAMPLE :\n",
      "difficulty mother's pardon, that! white-washed aloud Mr. pray, periodically particularly RELATES\n",
      "loss at epoch : 15  =  7.964372158050537\n",
      "SAMPLE :\n",
      "more dismal 'You, operation parochial all), home It's hour, months: 'Lor\n",
      "loss at epoch : 16  =  7.955472469329834\n",
      "SAMPLE :\n",
      "Dickens 'Yes, a sound his them. anon gin-and-water, food, it. his\n",
      "loss at epoch : 17  =  7.946254253387451\n",
      "SAMPLE :\n",
      "assisted circumstance call surrounded oratorical With hunger, compelling inheritance thrusting raising\n",
      "loss at epoch : 18  =  7.936692237854004\n",
      "SAMPLE :\n",
      "clearly 'Not It's too; wide all; bedstead, attention support rather body\n",
      "loss at epoch : 19  =  7.926756381988525\n",
      "SAMPLE :\n",
      "years. They officers for, THE profound influence tingling nourishment right. _they_\n",
      "loss at epoch : 20  =  7.916418075561523\n",
      "SAMPLE :\n",
      "neglect, waking service, ATTENDING woman wit, OF presuming 'Ah, lingering lump\n",
      "loss at epoch : 21  =  7.905645847320557\n",
      "SAMPLE :\n",
      "said reward woman Twist iron poised sneezed, stammered this, stooped business,'\n",
      "loss at epoch : 22  =  7.894417762756348\n",
      "SAMPLE :\n",
      "lighted is, proper say decidedly favour biography, advertise periodically bread cudgelling\n",
      "loss at epoch : 23  =  7.88270378112793\n",
      "SAMPLE :\n",
      "pocket might possessed inducing patchwork never created between befall inestimable nobody\n",
      "loss at epoch : 24  =  7.870459079742432\n",
      "SAMPLE :\n",
      "about say his is, into many said, BIRTH anciently bear many\n",
      "loss at epoch : 25  =  7.8576579093933105\n",
      "SAMPLE :\n",
      "reasonably these parish, male unequally circumstance OLIVER being appendage, mentioning, sneezed,\n",
      "loss at epoch : 26  =  7.844263076782227\n",
      "SAMPLE :\n",
      "talk hands at instance, towns, considerable quarter. CIRCUMSTANCES point born mattress,\n",
      "loss at epoch : 27  =  7.8302412033081055\n",
      "SAMPLE :\n",
      "much time. beer; never prefixed imposed PARISH BIRTH take OR an\n",
      "loss at epoch : 28  =  7.815558433532715\n",
      "SAMPLE :\n",
      "male BY part WHERE lungs, burden raised etext minutes common contract;\n",
      "loss at epoch : 29  =  7.800179481506348\n",
      "SAMPLE :\n",
      "like this iron expected editing faithful fire: die.' biography, poised great\n",
      "loss at epoch : 30  =  7.78407096862793\n",
      "SAMPLE :\n",
      "good for rustled; parish 'Let possessed 1 sorrow ATTENDING 10 possible\n",
      "loss at epoch : 31  =  7.767199516296387\n",
      "SAMPLE :\n",
      "did inevitably time an careful faithful child as his gave THE\n",
      "loss at epoch : 32  =  7.749557971954346\n",
      "SAMPLE :\n",
      "tried sense workhouse, part see Little. take and rose, occurred. faint\n",
      "loss at epoch : 33  =  7.7311201095581055\n",
      "SAMPLE :\n",
      "that! survive up small: proceeded repeat, inmates best will possibly had,\n",
      "loss at epoch : 34  =  7.7118964195251465\n",
      "SAMPLE :\n",
      "boys, year difficulty no feebly expected in buildings The his age\n",
      "loss at epoch : 35  =  7.6919074058532715\n",
      "SAMPLE :\n",
      "water; shone stray warm consequence breathed, TREATS much AND circumstance fire:\n",
      "loss at epoch : 36  =  7.67120361328125\n",
      "SAMPLE :\n",
      "small: part invariably astounded did face palms was, in matter imposed\n",
      "loss at epoch : 37  =  7.6498823165893555\n",
      "SAMPLE :\n",
      "sleep! away experience; awakens anything age THE being doubt voice am\n",
      "loss at epoch : 38  =  7.628108024597168\n",
      "SAMPLE :\n",
      "ATTENDING example yellow all), OF practice, of prudent is, other circumstance\n",
      "loss at epoch : 39  =  7.606100082397461\n",
      "SAMPLE :\n",
      "PLACE am As about born; inevitably Dickens grandmothers, workhouse, all careful\n",
      "loss at epoch : 40  =  7.584183216094971\n",
      "SAMPLE :\n",
      "OLIVER dismal generally workhouse; nurses, had do been small: whose doubt\n",
      "loss at epoch : 41  =  7.562736988067627\n",
      "SAMPLE :\n",
      "TWIST darkness, by Although matters events; HIS CHARLES unequally by assign\n",
      "loss at epoch : 42  =  7.542186260223389\n",
      "SAMPLE :\n",
      "editing 10 inducing time. respiration,--a workhouse; unwonted custom THE rendered woman,\n",
      "loss at epoch : 43  =  7.522857189178467\n",
      "SAMPLE :\n",
      "BORN 10 matters aunts, wit, many matter Next appeared; take latter.\n",
      "loss at epoch : 44  =  7.504814624786377\n",
      "SAMPLE :\n",
      "1 many OLIVER Little. CHAPTER rather himself wit, between chapter. world\n",
      "loss at epoch : 45  =  7.487761974334717\n",
      "SAMPLE :\n",
      "town, PARISH repeat, had, BIRTH BIRTH Among favour itself after DICKENS\n",
      "loss at epoch : 46  =  7.471065044403076\n",
      "SAMPLE :\n",
      "towns, Dickens Next Leigh stage poised has poised nurses, Index such\n",
      "loss at epoch : 47  =  7.454019069671631\n",
      "SAMPLE :\n",
      "BIRTH was repeat, case refrain THE never been FullBooks.com 11 mattress,\n",
      "loss at epoch : 48  =  7.436110496520996\n",
      "SAMPLE :\n",
      "CHARLES towns, BORN myself result some poised Nature gasping business is,\n",
      "loss at epoch : 49  =  7.417231559753418\n",
      "SAMPLE :\n",
      "PARISH OLIVER born; or WAS have memoirs literature easy appeared; merit\n",
      "loss at epoch : 50  =  7.397663116455078\n",
      "SAMPLE :\n",
      "Oliver DICKENS AND Among Oliver OLIVER Charles by Index within best\n",
      "loss at epoch : 51  =  7.377954959869385\n",
      "SAMPLE :\n",
      "or most workhouse; prudent OLIVER public the on Dickens BOY'S workhouse;\n",
      "loss at epoch : 52  =  7.358703136444092\n",
      "SAMPLE :\n",
      "town, to inasmuch OLIVER OLIVER in Peggy for CHARLES BOY'S Twist\n",
      "loss at epoch : 53  =  7.340332984924316\n",
      "SAMPLE :\n",
      "great reasons Oliver (2) from 11 business Leigh by CHARLES Among\n",
      "loss at epoch : 54  =  7.322963237762451\n",
      "SAMPLE :\n",
      "WAS at is common wit, Part (2) name, born; inasmuch CHARLES\n",
      "loss at epoch : 55  =  7.306421279907227\n",
      "SAMPLE :\n",
      "will TWIST PARISH of can anciently can Next out WHERE born;\n",
      "loss at epoch : 56  =  7.290335655212402\n",
      "SAMPLE :\n",
      "BIRTH Charles most public name, trouble assign Index Next wit, Little.\n",
      "loss at epoch : 57  =  7.274294853210449\n",
      "SAMPLE :\n",
      "wit, Leigh as prudent not Next the TWIST (2) for of\n",
      "loss at epoch : 58  =  7.257965087890625\n",
      "SAMPLE :\n",
      "many AND and out buildings BIRTH Leigh Twist created reasons on\n",
      "loss at epoch : 59  =  7.241160869598389\n",
      "SAMPLE :\n",
      "date THE OLIVER prudent WAS prudent consequence out not Among small:\n",
      "loss at epoch : 60  =  7.223827838897705\n",
      "SAMPLE :\n",
      "other wit, part name, inasmuch fictitious other in was Dickens date\n",
      "loss at epoch : 61  =  7.20606803894043\n",
      "SAMPLE :\n",
      "buildings Oliver a mentioning, is Leigh etext THE WAS workhouse stage\n",
      "loss at epoch : 62  =  7.188076496124268\n",
      "SAMPLE :\n",
      "etext BIRTH BOY'S TWIST Next WHERE can date editing Leigh as\n",
      "loss at epoch : 63  =  7.170068264007568\n",
      "SAMPLE :\n",
      "of reasons which fictitious possible from Peggy all business This 10\n",
      "loss at epoch : 64  =  7.152198314666748\n",
      "SAMPLE :\n",
      "from no day OF part not Gaugy. Edition was Peggy created\n",
      "loss at epoch : 65  =  7.13444185256958\n",
      "SAMPLE :\n",
      "consequence in at this I refrain PARISH public BIRTH need BY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch : 66  =  7.116554260253906\n",
      "SAMPLE :\n",
      "certain stage Among (2) TWIST reasons of was CIRCUMSTANCES This possible\n",
      "loss at epoch : 67  =  7.098197937011719\n",
      "SAMPLE :\n",
      "anciently TREATS no (2) small: not Part be consequence Edition which\n",
      "loss at epoch : 68  =  7.079169750213623\n",
      "SAMPLE :\n",
      "HIS which most this is this OF be Dickens buildings which\n",
      "loss at epoch : 69  =  7.059575080871582\n",
      "SAMPLE :\n",
      "prudent Oliver OLIVER consequence OF ATTENDING buildings Index assign is which\n",
      "loss at epoch : 70  =  7.039809226989746\n",
      "SAMPLE :\n",
      "anciently (2) part Leigh Gaugy. need towns, and myself consequence This\n",
      "loss at epoch : 71  =  7.020282745361328\n",
      "SAMPLE :\n",
      "BORN trouble BY public Among (2) buildings reader, name, Dickens reasons\n",
      "loss at epoch : 72  =  7.00117301940918\n",
      "SAMPLE :\n",
      "WHERE town, wit, this AND myself Leigh reader, wit, PROGRESS workhouse\n",
      "loss at epoch : 73  =  6.982322692871094\n",
      "SAMPLE :\n",
      "OR at TREATS HIS is other workhouse; editing Little. buildings or\n",
      "loss at epoch : 74  =  6.963362216949463\n",
      "SAMPLE :\n",
      "repeat, out stage a TREATS Twist Gaugy. WAS homepage small: ATTENDING\n",
      "loss at epoch : 75  =  6.943972587585449\n",
      "SAMPLE :\n",
      "reader, buildings from wit, TWIST ATTENDING most CIRCUMSTANCES PLACE as 11\n",
      "loss at epoch : 76  =  6.924046993255615\n",
      "SAMPLE :\n",
      "all 1 Little. was created buildings this town, Index Oliver can\n",
      "loss at epoch : 77  =  6.903742790222168\n",
      "SAMPLE :\n",
      "is one all at Leigh Leigh small: BY was one WAS\n",
      "loss at epoch : 78  =  6.883301734924316\n",
      "SAMPLE :\n",
      "BIRTH day mentioning, which there HIS BORN anciently Index was many\n",
      "loss at epoch : 79  =  6.862813472747803\n",
      "SAMPLE :\n",
      "Oliver DICKENS FullBooks.com homepage be OLIVER inasmuch Index can Twist was\n",
      "loss at epoch : 80  =  6.842018127441406\n",
      "SAMPLE :\n",
      "BOY'S Twist 11 OLIVER out Part common this day the business\n",
      "loss at epoch : 81  =  6.8205461502075195\n",
      "SAMPLE :\n",
      "day OF ATTENDING Leigh was towns, born; (2) Next no need\n",
      "loss at epoch : 82  =  6.79848051071167\n",
      "SAMPLE :\n",
      "consequence Index CIRCUMSTANCES PLACE TREATS Among CIRCUMSTANCES Part BIRTH and refrain\n",
      "loss at epoch : 83  =  6.776430606842041\n",
      "SAMPLE :\n",
      "day certain OLIVER prudent name, Part no certain was Oliver FullBooks.com\n",
      "loss at epoch : 84  =  6.754979610443115\n",
      "SAMPLE :\n",
      "1 and BIRTH reasons many PARISH reasons all OLIVER wit, consequence\n",
      "loss at epoch : 85  =  6.734074592590332\n",
      "SAMPLE :\n",
      "name, created Among of will mentioning, CIRCUMSTANCES (2) Peggy small: WAS\n",
      "loss at epoch : 86  =  6.713104724884033\n",
      "SAMPLE :\n",
      "be Part CIRCUMSTANCES reasons OF editing prudent buildings 1 as prudent\n",
      "loss at epoch : 87  =  6.691528797149658\n",
      "SAMPLE :\n",
      "by born; 11 PARISH at Index was certain to which Leigh\n",
      "loss at epoch : 88  =  6.669323444366455\n",
      "SAMPLE :\n",
      "need not was will assign to homepage most BY it other\n",
      "loss at epoch : 89  =  6.646755695343018\n",
      "SAMPLE :\n",
      "this certain Little. TREATS homepage reasons stage 1 PARISH Next anciently\n",
      "loss at epoch : 90  =  6.623831272125244\n",
      "SAMPLE :\n",
      "public it inasmuch on PARISH for THE there there which is\n",
      "loss at epoch : 91  =  6.600099086761475\n",
      "SAMPLE :\n",
      "CHAPTER workhouse at be no business Index trouble editing PROGRESS in\n",
      "loss at epoch : 92  =  6.575526714324951\n",
      "SAMPLE :\n",
      "inasmuch BOY'S TWIST OLIVER mentioning, FullBooks.com Part homepage stage BY Leigh\n",
      "loss at epoch : 93  =  6.551105499267578\n",
      "SAMPLE :\n",
      "reasons prudent 11 will reasons no PARISH BIRTH Peggy fictitious the\n",
      "loss at epoch : 94  =  6.527951240539551\n",
      "SAMPLE :\n",
      "there This most 10 (2) 1 by FullBooks.com Among the day\n",
      "loss at epoch : 95  =  6.505922794342041\n",
      "SAMPLE :\n",
      "Twist mentioning, homepage Among trouble PARISH repeat, Gaugy. towns, Peggy repeat,\n",
      "loss at epoch : 96  =  6.483792304992676\n",
      "SAMPLE :\n",
      "workhouse 11 AND 11 as Gaugy. repeat, common Dickens myself refrain\n",
      "loss at epoch : 97  =  6.4607157707214355\n",
      "SAMPLE :\n",
      "small: date 11 This Among 10 other born; AND myself which\n",
      "loss at epoch : 98  =  6.4369120597839355\n",
      "SAMPLE :\n",
      "etext the fictitious BORN Edition which most reasons 10 Gaugy. be\n",
      "loss at epoch : 99  =  6.412884712219238\n",
      "SAMPLE :\n",
      "wit, prudent HIS etext is of wit, there myself the a\n",
      "loss at epoch : 100  =  6.388045787811279\n",
      "SAMPLE :\n",
      "BORN (2) 11 HIS can great towns, Peggy on Dickens consequence\n",
      "loss at epoch : 101  =  6.36128044128418\n",
      "SAMPLE :\n",
      "common 10 small: there can buildings can PROGRESS Gaugy. towns, Peggy\n",
      "loss at epoch : 102  =  6.3336663246154785\n",
      "SAMPLE :\n",
      "is or fictitious DICKENS at Little. great ATTENDING or OR Charles\n",
      "loss at epoch : 103  =  6.307912826538086\n",
      "SAMPLE :\n",
      "BORN Gaugy. etext (2) Leigh FullBooks.com Little. myself WAS AND at\n",
      "loss at epoch : 104  =  6.284848213195801\n",
      "SAMPLE :\n",
      "ATTENDING PLACE Next I most fictitious certain anciently town, as OLIVER\n",
      "loss at epoch : 105  =  6.262435436248779\n",
      "SAMPLE :\n",
      "Gaugy. possible myself anciently PLACE date out Next one was is\n",
      "loss at epoch : 106  =  6.238651752471924\n",
      "SAMPLE :\n",
      "This public be will public Oliver a it reader, Part assign\n",
      "loss at epoch : 107  =  6.213701248168945\n",
      "SAMPLE :\n",
      "repeat, THE and town, not BY name, no will mentioning, BY\n",
      "loss at epoch : 108  =  6.18892765045166\n",
      "SAMPLE :\n",
      "Dickens 10 Peggy great most OR CIRCUMSTANCES many Index and name,\n",
      "loss at epoch : 109  =  6.164214134216309\n",
      "SAMPLE :\n",
      "stage editing OR refrain date WHERE part CHAPTER prudent one trouble\n",
      "loss at epoch : 110  =  6.136918544769287\n",
      "SAMPLE :\n",
      "other day reader, all HIS buildings reasons will towns, Among Index\n",
      "loss at epoch : 111  =  6.106562614440918\n",
      "SAMPLE :\n",
      "reader, town, other and date created this created one CHARLES possible\n",
      "loss at epoch : 112  =  6.077075481414795\n",
      "SAMPLE :\n",
      "town, Part AND town, all FullBooks.com public FullBooks.com Edition reasons is\n",
      "loss at epoch : 113  =  6.051607608795166\n",
      "SAMPLE :\n",
      "Oliver the TWIST towns, to wit, BOY'S BOY'S no common homepage\n",
      "loss at epoch : 114  =  6.028730869293213\n",
      "SAMPLE :\n",
      "WAS This WAS FullBooks.com inasmuch common reader, the this 1 BORN\n",
      "loss at epoch : 115  =  6.00536584854126\n",
      "SAMPLE :\n",
      "trouble Dickens etext OR Among other and Part Edition at THE\n",
      "loss at epoch : 116  =  5.980494022369385\n",
      "SAMPLE :\n",
      "a AND need it WAS created workhouse; be this OR as\n",
      "loss at epoch : 117  =  5.954679489135742\n",
      "SAMPLE :\n",
      "part out a town, OR part Little. PROGRESS Gaugy. THE 1\n",
      "loss at epoch : 118  =  5.928981781005859\n",
      "SAMPLE :\n",
      "for certain 1 as WHERE Little. BORN BIRTH BIRTH is the\n",
      "loss at epoch : 119  =  5.903720378875732\n",
      "SAMPLE :\n",
      "Dickens in many on towns, born; wit, business Edition as a\n",
      "loss at epoch : 120  =  5.875992298126221\n",
      "SAMPLE :\n",
      "OR Twist TREATS in Peggy Oliver Charles Edition PROGRESS (2) refrain\n",
      "loss at epoch : 121  =  5.843600273132324\n",
      "SAMPLE :\n",
      "OF Index be certain stage from repeat, Peggy editing on CIRCUMSTANCES\n",
      "loss at epoch : 122  =  5.810354709625244\n",
      "SAMPLE :\n",
      "it at editing consequence OF TWIST is stage or etext mentioning,\n",
      "loss at epoch : 123  =  5.781757354736328\n",
      "SAMPLE :\n",
      "myself CHAPTER TWIST at PLACE this all etext Dickens mentioning, PROGRESS\n",
      "loss at epoch : 124  =  5.7586798667907715\n",
      "SAMPLE :\n",
      "there workhouse; TREATS refrain common PLACE THE day homepage name, possible\n",
      "loss at epoch : 125  =  5.737659931182861\n",
      "SAMPLE :\n",
      "Next this Gaugy. Among Part buildings Peggy Twist created Oliver BIRTH\n",
      "loss at epoch : 126  =  5.714291572570801\n",
      "SAMPLE :\n",
      "TREATS 1 Dickens or other reasons by I for as Next\n",
      "loss at epoch : 127  =  5.686917304992676\n",
      "SAMPLE :\n",
      "great editing THE consequence small: OF or Gaugy. name, certain is\n",
      "loss at epoch : 128  =  5.658318996429443\n",
      "SAMPLE :\n",
      "WHERE OLIVER date Oliver fictitious public need This other anciently BY\n",
      "loss at epoch : 129  =  5.631838798522949\n",
      "SAMPLE :\n",
      "Next many no HIS OR by anciently 10 and in all\n",
      "loss at epoch : 130  =  5.606631278991699\n",
      "SAMPLE :\n",
      "workhouse Gaugy. I is date etext Oliver wit, and Little. BY\n",
      "loss at epoch : 131  =  5.577602863311768\n",
      "SAMPLE :\n",
      "this towns, the inasmuch the to towns, date Edition common TREATS\n",
      "loss at epoch : 132  =  5.542332172393799\n",
      "SAMPLE :\n",
      "created OLIVER PLACE and one the Twist for I Twist consequence\n",
      "loss at epoch : 133  =  5.506528854370117\n",
      "SAMPLE :\n",
      "1 HIS BIRTH will Twist business other Charles Dickens be TWIST\n",
      "loss at epoch : 134  =  5.477583408355713\n",
      "SAMPLE :\n",
      "homepage business town, WAS OF THE for Oliver Gaugy. This I\n",
      "loss at epoch : 135  =  5.455801010131836\n",
      "SAMPLE :\n",
      "fictitious there town, ATTENDING and BY Index CHAPTER of Little. day\n",
      "loss at epoch : 136  =  5.435516834259033\n",
      "SAMPLE :\n",
      "AND Dickens towns, (2) Little. 10 10 Among homepage PARISH fictitious\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch : 137  =  5.412140369415283\n",
      "SAMPLE :\n",
      "no DICKENS BY PROGRESS no not 1 date in ATTENDING most\n",
      "loss at epoch : 138  =  5.384805202484131\n",
      "SAMPLE :\n",
      "wit, this Dickens Next day Leigh which mentioning, is Leigh great\n",
      "loss at epoch : 139  =  5.3546600341796875\n",
      "SAMPLE :\n",
      "on mentioning, out part mentioning, by can from CHAPTER name, this\n",
      "loss at epoch : 140  =  5.324007034301758\n",
      "SAMPLE :\n",
      "CIRCUMSTANCES small: name, trouble AND BORN PARISH DICKENS a fictitious Gaugy.\n",
      "loss at epoch : 141  =  5.295896530151367\n",
      "SAMPLE :\n",
      "was WHERE other refrain common CIRCUMSTANCES not PARISH all Twist DICKENS\n",
      "loss at epoch : 142  =  5.270724296569824\n",
      "SAMPLE :\n",
      "1 inasmuch created CHARLES wit, one OF date PARISH in reasons\n",
      "loss at epoch : 143  =  5.243423938751221\n",
      "SAMPLE :\n",
      "PARISH Little. public CIRCUMSTANCES no 1 OR OF repeat, which OF\n",
      "loss at epoch : 144  =  5.209590911865234\n",
      "SAMPLE :\n",
      "(2) PLACE Leigh Index a WAS homepage FullBooks.com Gaugy. myself PROGRESS\n",
      "loss at epoch : 145  =  5.173271179199219\n",
      "SAMPLE :\n",
      "Twist inasmuch it day out prudent homepage small: Little. part possible\n",
      "loss at epoch : 146  =  5.142130374908447\n",
      "SAMPLE :\n",
      "of stage this which BY refrain Peggy Edition workhouse; workhouse; PARISH\n",
      "loss at epoch : 147  =  5.118136405944824\n",
      "SAMPLE :\n",
      "out part as will Oliver most WHERE need Among repeat, CIRCUMSTANCES\n",
      "loss at epoch : 148  =  5.096934795379639\n",
      "SAMPLE :\n",
      "Gaugy. day created on 11 I wit, AND towns, at and\n",
      "loss at epoch : 149  =  5.073530673980713\n",
      "SAMPLE :\n",
      "it by part common refrain BORN editing Peggy Next business Edition\n",
      "loss at epoch : 150  =  5.04624080657959\n",
      "SAMPLE :\n",
      "date Next born; PROGRESS Among OF Oliver one small: repeat, BOY'S\n",
      "loss at epoch : 151  =  5.016350746154785\n",
      "SAMPLE :\n",
      "common or will mentioning, PROGRESS Little. Index DICKENS born; reader, possible\n",
      "loss at epoch : 152  =  4.98599100112915\n",
      "SAMPLE :\n",
      "all Edition 11 can BY WAS OR possible Gaugy. inasmuch town,\n",
      "loss at epoch : 153  =  4.957070350646973\n",
      "SAMPLE :\n",
      "CHARLES day can myself Peggy editing Peggy all towns, Little. Edition\n",
      "loss at epoch : 154  =  4.931033134460449\n",
      "SAMPLE :\n",
      "Little. stage as not workhouse the Next or Dickens name, town,\n",
      "loss at epoch : 155  =  4.906534194946289\n",
      "SAMPLE :\n",
      "mentioning, editing Twist anciently was TWIST one PLACE 11 there FullBooks.com\n",
      "loss at epoch : 156  =  4.878420352935791\n",
      "SAMPLE :\n",
      "Twist created I Little. Little. a Twist repeat, repeat, not ATTENDING\n",
      "loss at epoch : 157  =  4.843532562255859\n",
      "SAMPLE :\n",
      "be towns, part no for mentioning, DICKENS workhouse; to homepage Little.\n",
      "loss at epoch : 158  =  4.805903434753418\n",
      "SAMPLE :\n",
      "the prudent can is repeat, 10 for PARISH date This prudent\n",
      "loss at epoch : 159  =  4.772731781005859\n",
      "SAMPLE :\n",
      "one town, buildings the Edition OF BIRTH 1 BIRTH common etext\n",
      "loss at epoch : 160  =  4.747125625610352\n",
      "SAMPLE :\n",
      "CHAPTER (2) WAS Index day for repeat, BOY'S other of town,\n",
      "loss at epoch : 161  =  4.73094367980957\n",
      "SAMPLE :\n",
      "Charles 1 Twist Peggy BORN THE most public date towns, AND\n",
      "loss at epoch : 162  =  4.713157653808594\n",
      "SAMPLE :\n",
      "ATTENDING PLACE in OR out trouble as all TWIST PARISH for\n",
      "loss at epoch : 163  =  4.692145824432373\n",
      "SAMPLE :\n",
      "mentioning, this buildings OF TWIST 1 editing This created out Gaugy.\n",
      "loss at epoch : 164  =  4.66445255279541\n",
      "SAMPLE :\n",
      "Next WAS public public reader, PLACE this Next etext of OLIVER\n",
      "loss at epoch : 165  =  4.631824016571045\n",
      "SAMPLE :\n",
      "there or can created OF the (2) BIRTH no out editing\n",
      "loss at epoch : 166  =  4.599417686462402\n",
      "SAMPLE :\n",
      "common editing which I there out was editing other BOY'S one\n",
      "loss at epoch : 167  =  4.572324275970459\n",
      "SAMPLE :\n",
      "mentioning, was towns, This Little. be date by out Dickens This\n",
      "loss at epoch : 168  =  4.551201343536377\n",
      "SAMPLE :\n",
      "BORN FullBooks.com PROGRESS homepage be the part Gaugy. workhouse can no\n",
      "loss at epoch : 169  =  4.530299186706543\n",
      "SAMPLE :\n",
      "will assign is TWIST was WHERE inasmuch Gaugy. it workhouse workhouse\n",
      "loss at epoch : 170  =  4.502772808074951\n",
      "SAMPLE :\n",
      "from on OR Twist CHARLES of small: anciently I WAS DICKENS\n",
      "loss at epoch : 171  =  4.468500137329102\n",
      "SAMPLE :\n",
      "business PROGRESS business at myself it CHARLES towns, on Charles 1\n",
      "loss at epoch : 172  =  4.434398651123047\n",
      "SAMPLE :\n",
      "great assign not TWIST PROGRESS Among Leigh possible (2) CIRCUMSTANCES there\n",
      "loss at epoch : 173  =  4.406286239624023\n",
      "SAMPLE :\n",
      "BOY'S be in mentioning, public date date anciently date all (2)\n",
      "loss at epoch : 174  =  4.3838958740234375\n",
      "SAMPLE :\n",
      "no workhouse OR in wit, need Little. can there possible business\n",
      "loss at epoch : 175  =  4.363040447235107\n",
      "SAMPLE :\n",
      "and assign date (2) wit, the can was at PARISH date\n",
      "loss at epoch : 176  =  4.339817047119141\n",
      "SAMPLE :\n",
      "BY reasons there CHARLES stage mentioning, at wit, small: can TWIST\n",
      "loss at epoch : 177  =  4.313056945800781\n",
      "SAMPLE :\n",
      "the etext Index there CHAPTER a CHAPTER Twist repeat, BOY'S towns,\n",
      "loss at epoch : 178  =  4.284317493438721\n",
      "SAMPLE :\n",
      "buildings ATTENDING THE PLACE HIS 1 Part homepage HIS repeat, Peggy\n",
      "loss at epoch : 179  =  4.25620174407959\n",
      "SAMPLE :\n",
      "certain as common assign reader, no repeat, created great anciently by\n",
      "loss at epoch : 180  =  4.230424404144287\n",
      "SAMPLE :\n",
      "the etext town, BY OR reasons BORN part most Charles Oliver\n",
      "loss at epoch : 181  =  4.206805229187012\n",
      "SAMPLE :\n",
      "prudent BORN all OLIVER out this reasons PROGRESS workhouse CHARLES is\n",
      "loss at epoch : 182  =  4.183547019958496\n",
      "SAMPLE :\n",
      "FullBooks.com assign out most certain one I to Index CIRCUMSTANCES be\n",
      "loss at epoch : 183  =  4.158459663391113\n",
      "SAMPLE :\n",
      "date HIS which myself public not consequence Index 1 refrain public\n",
      "loss at epoch : 184  =  4.130167007446289\n",
      "SAMPLE :\n",
      "Gaugy. date Little. part most assign other Edition Among DICKENS of\n",
      "loss at epoch : 185  =  4.099421501159668\n",
      "SAMPLE :\n",
      "to will editing repeat, TWIST BOY'S BIRTH BORN which as the\n",
      "loss at epoch : 186  =  4.068815231323242\n",
      "SAMPLE :\n",
      "was This Index OLIVER many CHARLES prudent PROGRESS from refrain Dickens\n",
      "loss at epoch : 187  =  4.040707588195801\n",
      "SAMPLE :\n",
      "certain it was THE great born; PLACE Among from common all\n",
      "loss at epoch : 188  =  4.015769958496094\n",
      "SAMPLE :\n",
      "not workhouse be mentioning, possible CHARLES will Next TWIST 1 trouble\n",
      "loss at epoch : 189  =  3.9931747913360596\n",
      "SAMPLE :\n",
      "inasmuch refrain homepage Oliver I not the fictitious anciently 11 OLIVER\n",
      "loss at epoch : 190  =  3.9714653491973877\n",
      "SAMPLE :\n",
      "it I great was BOY'S TREATS Index Dickens for for reader,\n",
      "loss at epoch : 191  =  3.949162244796753\n",
      "SAMPLE :\n",
      "or born; Twist Peggy was refrain BY certain will and Twist\n",
      "loss at epoch : 192  =  3.925222873687744\n",
      "SAMPLE :\n",
      "created mentioning, many of on Charles no BIRTH buildings CIRCUMSTANCES which\n",
      "loss at epoch : 193  =  3.899585008621216\n",
      "SAMPLE :\n",
      "or stage TWIST TREATS Little. for BY workhouse and most Gaugy.\n",
      "loss at epoch : 194  =  3.8734819889068604\n",
      "SAMPLE :\n",
      "public Dickens prudent can BOY'S FullBooks.com Dickens Part Charles PROGRESS myself\n",
      "loss at epoch : 195  =  3.8488335609436035\n",
      "SAMPLE :\n",
      "This PLACE PLACE all day to Next Next need which other\n",
      "loss at epoch : 196  =  3.8258056640625\n",
      "SAMPLE :\n",
      "this public of inasmuch all certain be can workhouse; name, prudent\n",
      "loss at epoch : 197  =  3.802503824234009\n",
      "SAMPLE :\n",
      "possible OR BIRTH born; Oliver WAS AND born; day OLIVER CHAPTER\n",
      "loss at epoch : 198  =  3.7763636112213135\n",
      "SAMPLE :\n",
      "or myself as for will I CHARLES business ATTENDING towns, I\n",
      "loss at epoch : 199  =  3.747013807296753\n",
      "SAMPLE :\n",
      "This name, BY WAS repeat, out prudent on prudent Index workhouse\n",
      "loss at epoch : 200  =  3.717008352279663\n",
      "SAMPLE :\n",
      "workhouse DICKENS Index 11 can was Leigh day reader, out THE\n",
      "loss at epoch : 201  =  3.689725160598755\n",
      "SAMPLE :\n",
      "Oliver it THE I Oliver by and trouble towns, CHARLES to\n",
      "loss at epoch : 202  =  3.6668455600738525\n",
      "SAMPLE :\n",
      "I will born; one homepage to in Dickens OR no as\n",
      "loss at epoch : 203  =  3.6474080085754395\n",
      "SAMPLE :\n",
      "can reader, towns, consequence stage OF TREATS myself it most Peggy\n",
      "loss at epoch : 204  =  3.6286590099334717\n",
      "SAMPLE :\n",
      "homepage 10 which assign at great BIRTH reader, ATTENDING great or\n",
      "loss at epoch : 205  =  3.607929229736328\n",
      "SAMPLE :\n",
      "and day Gaugy. anciently Next WAS not town, business I wit,\n",
      "loss at epoch : 206  =  3.5843594074249268\n",
      "SAMPLE :\n",
      "reasons need 10 and CIRCUMSTANCES workhouse; refrain there TWIST Twist PLACE\n",
      "loss at epoch : 207  =  3.5591964721679688\n",
      "SAMPLE :\n",
      "business WAS WHERE FullBooks.com PLACE mentioning, part consequence many WAS consequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch : 208  =  3.534588098526001\n",
      "SAMPLE :\n",
      "1 reader, in THE assign reader, be at or small: no\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 6.00 GiB total capacity; 3.87 GiB already allocated; 119.06 MiB free; 4.19 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-56c0db27a839>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrained_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprev_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0miter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-69cd5f10cbe4>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(prev_state, model, optimizer, criterion, dataset, iter)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0myhat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprev_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprev_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-bdca7bab99a7>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, prev_state)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m#don't specify a_out ,self.hidden = self.lstm(embedded_vector.view(self.batch_size,self.sequance_length, -1),self.hidden)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mdense\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0moutput_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedded_vector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membedded_vector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput_scores\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprev_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1315\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'log_softmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1317\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 6.00 GiB total capacity; 3.87 GiB already allocated; 119.06 MiB free; 4.19 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(prev_state,model,optimizer,criterion,data_set,iter = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trained_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-27652d883ac7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SAMPLE : \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"the\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trained_model' is not defined"
     ]
    }
   ],
   "source": [
    "print(trained_model.parameters())\n",
    "print(\"SAMPLE : \",predict(\"the\".split(),100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
