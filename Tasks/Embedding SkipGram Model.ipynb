{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1979e730170>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "from torch import nn,optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerToTensor():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self,x):\n",
    "        return torch.tensor(x,dtype=torch.long).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusDataset(Dataset):\n",
    "    def __init__(self,corpus,context_size = 2,transform = None):\n",
    "        self.transform = transform\n",
    "        self.context_size = context_size\n",
    "        self.tokens = self.tokenize_corpus(corpus)\n",
    "        self.vocabulary = self.get_vocabulary(self.tokens)\n",
    "        self.word2idx = self.word2idx(self.vocabulary)\n",
    "        self.idx2word = self.idx2word(self.vocabulary)     \n",
    "        self.data = self.context_vector(self.tokens)\n",
    "        self.dataset = self.idx_pair(self.data)\n",
    "\n",
    "            \n",
    "    def get_vocabulary(self,tokens):\n",
    "        vocabulary = []\n",
    "        for sentence in tokens:\n",
    "            for token in sentence:\n",
    "                if token not in vocabulary:\n",
    "                    vocabulary.append(token)\n",
    "        return vocabulary\n",
    "    \n",
    "    def word2idx(self,vocabulary):\n",
    "        return {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "    \n",
    "    def idx2word(self,vocabulary):\n",
    "        return {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "    \n",
    "    def tokenize_corpus(self,corpus):\n",
    "        tokens = [x.split() for x in corpus]\n",
    "        return tokens\n",
    "    \n",
    "    def context_vector(self,tokens):\n",
    "        data = []\n",
    "        for sentence in tokens:\n",
    "            for i in range(0,len(sentence)): #min len(sentence) = 2\n",
    "                context = []\n",
    "                for j in reversed(range(1,self.context_size+1)):\n",
    "                    if((i-j) >= 0): #if sentence[i-j] != null\n",
    "                        context.append(sentence[i-j])\n",
    "                for j in range(1,self.context_size+1):\n",
    "                    if((i+j) < len(sentence)): #if sentence[i+j] != null\n",
    "                        context.append(sentence[i+j])\n",
    "                target = sentence[i]\n",
    "                data.append((context, target))\n",
    "        return data\n",
    "    \n",
    "    def idx_pair(self,data):\n",
    "        dataset = []\n",
    "        for target_sent,context_word in data:\n",
    "            for target_word in target_sent:\n",
    "                dataset.append([self.word2idx[context_word],self.word2idx[target_word]])\n",
    "        return dataset\n",
    "    \n",
    "    def show_dataset(self):\n",
    "        for input_word,output_word in self.dataset:\n",
    "            print(self.idx2word[input_word],self.idx2word[output_word])\n",
    "            \n",
    "    def __getitem__(self,idx):\n",
    "        x,y = self.dataset[idx]\n",
    "        if(self.transform):\n",
    "#              x = torch.tensor(x,dtype=torch.long)\n",
    "               x = self.transform(x)\n",
    "               y = self.transform(y)\n",
    "               #no need no transform y to tensor DataLoader will convert it to y = self.transform([y])\n",
    "        return x,y \n",
    "        #it must be returned like to to be casting as set for dataLoader\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "X = index from vocab 0 or 1 or 2 , .... , len(vocab)\n",
    "V = W1[x] \n",
    "out = dot(V,W2)\n",
    "out = softmax(out)\n",
    "W1.shape = (VxD)\n",
    "W2.shape = (DxV)\n",
    "V = (dx1)\n",
    "'''\n",
    "class word2vec(nn.Module):\n",
    "    def __init__(self,vocab_size,word_dims): #VxD\n",
    "        super(word2vec,self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size,word_dims)\n",
    "        self.linear = nn.Linear(word_dims,vocab_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        W1 = torch.relu(self.embed(x))\n",
    "        W2 = self.linear(W1)\n",
    "        out = torch.log_softmax(W2,dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus2 = [\"\"\"We are about to study the idea of a computational process.\n",
    "She me are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\"]\n",
    "data_set = CorpusDataset(corpus2,context_size = 2,transform = TransformerToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input =  tensor([0])  \n",
      "y =  tensor([1])\n",
      "tensor([[0.0000, 0.5319, 0.0000, 0.8346, 0.0000, 1.7922, 0.0000, 2.2565, 0.8924,\n",
      "         0.0146]], grad_fn=<ReluBackward0>)\n",
      "yhat =  tensor([[-4.8989, -4.1912, -3.0938, -4.7501, -3.3991, -4.0072, -4.5861, -4.0094,\n",
      "         -4.0515, -3.3863, -4.1836, -3.6497, -4.4531, -4.3826, -3.7989, -4.7417,\n",
      "         -3.4342, -4.8165, -4.5119, -4.3071, -4.8730, -4.4159, -3.1762, -2.9388,\n",
      "         -4.0777, -3.5504, -4.1216, -3.3930, -4.1421, -4.2582, -4.1913, -3.4914,\n",
      "         -3.8932, -2.7442, -4.9879, -3.8065, -4.2943, -4.3899, -4.5250, -3.8943,\n",
      "         -3.9443, -3.8186, -3.9188, -5.2185, -2.9947, -4.1476, -4.4780, -4.0423,\n",
      "         -5.2974, -4.8115]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "#testing input/output\n",
    "vocab_size = len(data_set.vocabulary)\n",
    "word_embed_dim = 10\n",
    "model = word2vec(vocab_size,word_embed_dim)\n",
    "print(\"input = \",data_set[0][0], \" \\ny = \",data_set[0][1])\n",
    "yhat = model(data_set[0][0])\n",
    "print(\"yhat = \",yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,criterion,optimizer,train_loader,iter=100):\n",
    "    for epoch in range(iter):\n",
    "        for x,y in train_loader:\n",
    "            yhat = model(x)\n",
    "            loss = criterion(yhat,y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if(epoch%100 == 0):\n",
    "            print(\"Epoch #\",epoch , \" Loss = \",loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus1 = [\n",
    "    'he is a king',\n",
    "    'she is a queen',\n",
    "    'he is a man',\n",
    "    'she is a woman',\n",
    "    'warsaw is poland capital',\n",
    "    'berlin is germany capital',\n",
    "    'paris is france capital',\n",
    "]\n",
    "corpus2 = [\"\"\"We are about to study the idea of a computational process.\n",
    "She me are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\"]\n",
    "\n",
    "corpus3 = [\"The earth revolves around the sun. The moon revolves around the earth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  tensor([0])  \n",
      "y=  tensor([1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_set = CorpusDataset(corpus3,context_size = 2,transform = TransformerToTensor())\n",
    "train_loader = DataLoader(dataset = data_set,batch_size=32)\n",
    "for x,y in train_loader:\n",
    "    print(\"x = \",x[0],\" \\ny= \",y[0])\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 0  Loss =  3.576582908630371\n",
      "Epoch # 100  Loss =  3.644570827484131\n",
      "Epoch # 200  Loss =  3.5677151679992676\n",
      "Epoch # 300  Loss =  3.435227870941162\n",
      "Epoch # 400  Loss =  3.2720112800598145\n",
      "Epoch # 500  Loss =  3.0820553302764893\n",
      "Epoch # 600  Loss =  2.8649721145629883\n",
      "Epoch # 700  Loss =  2.6252787113189697\n",
      "Epoch # 800  Loss =  2.377237558364868\n",
      "Epoch # 900  Loss =  2.139230728149414\n",
      "Epoch # 1000  Loss =  1.924010157585144\n",
      "Epoch # 1100  Loss =  1.7381389141082764\n",
      "Epoch # 1200  Loss =  1.5817519426345825\n",
      "Epoch # 1300  Loss =  1.451515555381775\n",
      "Epoch # 1400  Loss =  1.3430540561676025\n",
      "Epoch # 1500  Loss =  1.2523380517959595\n",
      "Epoch # 1600  Loss =  1.176454782485962\n",
      "Epoch # 1700  Loss =  1.1130121946334839\n",
      "Epoch # 1800  Loss =  1.0600882768630981\n",
      "Epoch # 1900  Loss =  1.0161319971084595\n",
      "Epoch # 2000  Loss =  0.9798581004142761\n",
      "Epoch # 2100  Loss =  0.9500794410705566\n",
      "Epoch # 2200  Loss =  0.9257381558418274\n",
      "Epoch # 2300  Loss =  0.9060798287391663\n",
      "Epoch # 2400  Loss =  0.8901686072349548\n",
      "Epoch # 2500  Loss =  0.8771320581436157\n",
      "Epoch # 2600  Loss =  0.866271436214447\n",
      "Epoch # 2700  Loss =  0.8570767641067505\n",
      "Epoch # 2800  Loss =  0.8493015766143799\n",
      "Epoch # 2900  Loss =  0.8426201939582825\n",
      "Epoch # 3000  Loss =  0.8367131352424622\n",
      "Epoch # 3100  Loss =  0.8313435316085815\n",
      "Epoch # 3200  Loss =  0.8265231847763062\n",
      "Epoch # 3300  Loss =  0.8221450448036194\n",
      "Epoch # 3400  Loss =  0.8180202841758728\n",
      "Epoch # 3500  Loss =  0.8140885829925537\n",
      "Epoch # 3600  Loss =  0.8104210495948792\n",
      "Epoch # 3700  Loss =  0.8068548440933228\n",
      "Epoch # 3800  Loss =  0.8035116791725159\n",
      "Epoch # 3900  Loss =  0.8002220392227173\n",
      "Epoch # 4000  Loss =  0.7971091270446777\n",
      "Epoch # 4100  Loss =  0.7941573858261108\n",
      "Epoch # 4200  Loss =  0.7912557721138\n",
      "Epoch # 4300  Loss =  0.7885431051254272\n",
      "Epoch # 4400  Loss =  0.7860078811645508\n",
      "Epoch # 4500  Loss =  0.7835365533828735\n",
      "Epoch # 4600  Loss =  0.7814455628395081\n",
      "Epoch # 4700  Loss =  0.7795748710632324\n",
      "Epoch # 4800  Loss =  0.7775920033454895\n",
      "Epoch # 4900  Loss =  0.7755703330039978\n",
      "Epoch # 5000  Loss =  0.7736873626708984\n",
      "Epoch # 5100  Loss =  0.7718713283538818\n",
      "Epoch # 5200  Loss =  0.7701146602630615\n",
      "Epoch # 5300  Loss =  0.7684222459793091\n",
      "Epoch # 5400  Loss =  0.7668327689170837\n",
      "Epoch # 5500  Loss =  0.7653201222419739\n",
      "Epoch # 5600  Loss =  0.7639132142066956\n",
      "Epoch # 5700  Loss =  0.7626448273658752\n",
      "Epoch # 5800  Loss =  0.7614884376525879\n",
      "Epoch # 5900  Loss =  0.7601965069770813\n",
      "Epoch # 6000  Loss =  0.7590354084968567\n",
      "Epoch # 6100  Loss =  0.7579428553581238\n",
      "Epoch # 6200  Loss =  0.7568964958190918\n",
      "Epoch # 6300  Loss =  0.7560098767280579\n",
      "Epoch # 6400  Loss =  0.7551012635231018\n",
      "Epoch # 6500  Loss =  0.7541971802711487\n",
      "Epoch # 6600  Loss =  0.7532681226730347\n",
      "Epoch # 6700  Loss =  0.7523791193962097\n",
      "Epoch # 6800  Loss =  0.7515743970870972\n",
      "Epoch # 6900  Loss =  0.7508352398872375\n",
      "Epoch # 7000  Loss =  0.7501325607299805\n",
      "Epoch # 7100  Loss =  0.7495031356811523\n",
      "Epoch # 7200  Loss =  0.7488954663276672\n",
      "Epoch # 7300  Loss =  0.748174786567688\n",
      "Epoch # 7400  Loss =  0.7475034594535828\n",
      "Epoch # 7500  Loss =  0.7467747330665588\n",
      "Epoch # 7600  Loss =  0.7461594343185425\n",
      "Epoch # 7700  Loss =  0.7456141114234924\n",
      "Epoch # 7800  Loss =  0.7450451254844666\n",
      "Epoch # 7900  Loss =  0.7445481419563293\n",
      "Epoch # 8000  Loss =  0.7439848780632019\n",
      "Epoch # 8100  Loss =  0.7434414029121399\n",
      "Epoch # 8200  Loss =  0.7430446147918701\n",
      "Epoch # 8300  Loss =  0.7425656318664551\n",
      "Epoch # 8400  Loss =  0.742097795009613\n",
      "Epoch # 8500  Loss =  0.7416313886642456\n",
      "Epoch # 8600  Loss =  0.7411232590675354\n",
      "Epoch # 8700  Loss =  0.7406792044639587\n",
      "Epoch # 8800  Loss =  0.7401438355445862\n",
      "Epoch # 8900  Loss =  0.7396568655967712\n",
      "Epoch # 9000  Loss =  0.7392104864120483\n",
      "Epoch # 9100  Loss =  0.7388398051261902\n",
      "Epoch # 9200  Loss =  0.7384111881256104\n",
      "Epoch # 9300  Loss =  0.7378882169723511\n",
      "Epoch # 9400  Loss =  0.7374716401100159\n",
      "Epoch # 9500  Loss =  0.7371053695678711\n",
      "Epoch # 9600  Loss =  0.7366947531700134\n",
      "Epoch # 9700  Loss =  0.7363187670707703\n",
      "Epoch # 9800  Loss =  0.7359068393707275\n",
      "Epoch # 9900  Loss =  0.7356013059616089\n"
     ]
    }
   ],
   "source": [
    "data_set = CorpusDataset(corpus2,context_size = 2,transform = TransformerToTensor())\n",
    "# train_loader = DataLoader(dataset = data_set,batch_size=32)\n",
    "#dataloader with batch_size > 1 make error with Embedded matrix\n",
    "vocab_size = len(data_set.vocabulary)\n",
    "word_embed_dim = 10\n",
    "model = word2vec(vocab_size,word_embed_dim)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)\n",
    "criterion = nn.NLLLoss()\n",
    "train_model(model,criterion,optimizer,data_set,iter=10000)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You can choose your embedded matrix as W1 or W2 or (W+W`)/2\n",
    "but i will choose W1 here\n",
    "note that :\n",
    "W1 = word2vec.embed\n",
    "W2 = word2vec.linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 10])\n",
      "torch.Size([50, 10])\n"
     ]
    }
   ],
   "source": [
    "#print(model.state_dict()['embed.weight'].size())\n",
    "W1 = model.state_dict()['embed.weight'].clone().detach()\n",
    "W2 = model.state_dict()['linear.weight'].clone().detach()\n",
    "print(W1.shape)\n",
    "print(W2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_words(word,top_k,Weights):\n",
    "    idx = torch.tensor(data_set.word2idx[word])\n",
    "    #W1(VxD) dot word_vec(Dx1) = Vx1 \n",
    "    word_vec = torch.tensor(Weights[idx]).view(-1,1)\n",
    "    yhat = torch.mm(Weights,word_vec)\n",
    "    values , most_similar_idx =  torch.topk(yhat, k=top_k, dim=0)\n",
    "    for i in range(top_k):\n",
    "        most_similar_word = data_set.idx2word[most_similar_idx[i].item()]\n",
    "        if(most_similar_word == word):\n",
    "            continue\n",
    "        print(most_similar_word,values[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We are about to study the idea of a computational process.\\nShe me are abstract beings that inhabit computers.\\nAs they evolve, processes manipulate other abstract things called data.\\nThe evolution of a process is directed by a pattern of rules\\ncalled a program. People create programs to direct processes. In effect,\\nwe conjure the spirits of the computer with our spells.']\n",
      "\n",
      "With weights W1 (nn.Embedded)\n",
      "\n",
      "processes 22.991008758544922\n",
      "a 19.313079833984375\n",
      "are 14.851171493530273\n",
      "\n",
      "With Weights W2 (nn.Linear)\n",
      "\n",
      "other 23.78009033203125\n",
      "processes 23.505077362060547\n",
      "abstract 19.477222442626953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muata\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "print(corpus2)\n",
    "word = \"manipulate\"\n",
    "print(\"\\nWith weights W1 (nn.Embedded)\\n\")\n",
    "get_similar_words(word,top_k=4,Weights=W1)\n",
    "print(\"\\nWith Weights W2 (nn.Linear)\\n\")\n",
    "get_similar_words(word,top_k=4,Weights=W2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "why there is two matrices W1 , W2?\n",
    "answer:\n",
    "https://stackoverflow.com/questions/29381505/why-does-word2vec-use-2-representations-for-each-word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
