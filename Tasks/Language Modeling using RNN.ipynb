{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2b6cba3f190>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-0.5525,  0.6355, -0.3968]]), tensor([[-0.6571, -1.6428,  0.9803]]), tensor([[-0.0421, -0.8206,  0.3133]]), tensor([[-1.1352,  0.3773, -0.2824]]), tensor([[-2.5667, -1.4303,  0.5009]])]\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(3, 3) #input word embedding in 3 dimension  ,hidden_state in 3 dim hint: (a) and (c)\n",
    "inputs1 = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5 Tx = 5 = Ty\n",
    "print(inputs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs =  [tensor([[-0.5525,  0.6355, -0.3968]]), tensor([[-0.6571, -1.6428,  0.9803]]), tensor([[-0.0421, -0.8206,  0.3133]]), tensor([[-1.1352,  0.3773, -0.2824]]), tensor([[-2.5667, -1.4303,  0.5009]])]\n",
      "\n",
      "hidden =  (tensor([[[ 0.5438, -0.4057,  1.1341]]]), tensor([[[-1.1115,  0.3501, -0.7703]]]))\n",
      "\n",
      " out at iter : 1  =  tensor([[[-0.2682,  0.0304, -0.1526]]], grad_fn=<StackBackward>)\n",
      "\n",
      " hidden at iter : 1  =  (tensor([[[-0.2682,  0.0304, -0.1526]]], grad_fn=<StackBackward>), tensor([[[-1.0766,  0.0972, -0.5498]]], grad_fn=<StackBackward>))\n",
      "\n",
      " out at iter : 2  =  tensor([[[-0.5370,  0.0346, -0.1958]]], grad_fn=<StackBackward>)\n",
      "\n",
      " hidden at iter : 2  =  (tensor([[[-0.5370,  0.0346, -0.1958]]], grad_fn=<StackBackward>), tensor([[[-1.1552,  0.1214, -0.2974]]], grad_fn=<StackBackward>))\n",
      "\n",
      " out at iter : 3  =  tensor([[[-0.3947,  0.0391, -0.1217]]], grad_fn=<StackBackward>)\n",
      "\n",
      " hidden at iter : 3  =  (tensor([[[-0.3947,  0.0391, -0.1217]]], grad_fn=<StackBackward>), tensor([[[-1.0727,  0.1104, -0.2179]]], grad_fn=<StackBackward>))\n",
      "\n",
      " out at iter : 4  =  tensor([[[-0.1854,  0.0740, -0.0979]]], grad_fn=<StackBackward>)\n",
      "\n",
      " hidden at iter : 4  =  (tensor([[[-0.1854,  0.0740, -0.0979]]], grad_fn=<StackBackward>), tensor([[[-1.0530,  0.1836, -0.1731]]], grad_fn=<StackBackward>))\n",
      "\n",
      " out at iter : 5  =  tensor([[[-0.3600,  0.0893,  0.0215]]], grad_fn=<StackBackward>)\n",
      "\n",
      " hidden at iter : 5  =  (tensor([[[-0.3600,  0.0893,  0.0215]]], grad_fn=<StackBackward>), tensor([[[-1.1298,  0.4467,  0.0254]]], grad_fn=<StackBackward>))\n",
      "\n",
      "inputs =  tensor([[[-0.5525,  0.6355, -0.3968]],\n",
      "\n",
      "        [[-0.6571, -1.6428,  0.9803]],\n",
      "\n",
      "        [[-0.0421, -0.8206,  0.3133]],\n",
      "\n",
      "        [[-1.1352,  0.3773, -0.2824]],\n",
      "\n",
      "        [[-2.5667, -1.4303,  0.5009]]])\n",
      "\n",
      "hidden =  (tensor([[[ 0.5438, -0.4057,  1.1341]]]), tensor([[[-1.1115,  0.3501, -0.7703]]]))\n",
      "\n",
      "out =  tensor([[[-0.2682,  0.0304, -0.1526]],\n",
      "\n",
      "        [[-0.5370,  0.0346, -0.1958]],\n",
      "\n",
      "        [[-0.3947,  0.0391, -0.1217]],\n",
      "\n",
      "        [[-0.1854,  0.0740, -0.0979]],\n",
      "\n",
      "        [[-0.3600,  0.0893,  0.0215]]], grad_fn=<StackBackward>)\n",
      "\n",
      "\n",
      " hidden =  (tensor([[[-0.3600,  0.0893,  0.0215]]], grad_fn=<StackBackward>), tensor([[[-1.1298,  0.4467,  0.0254]]], grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "# initialize the hidden state.\n",
    "hidden1 = (torch.randn(1, 1, 3),\n",
    "          torch.randn(1, 1, 3))\n",
    "hidden2 = hidden1\n",
    "x = 1\n",
    "print(\"inputs = \",inputs1)\n",
    "print(\"\\nhidden = \",hidden1)\n",
    "for i in inputs1:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out1, hidden1 = lstm(i.view(1, 1, -1), hidden1)\n",
    "    print(\"\\n out at iter :\",x ,\" = \" ,out1)\n",
    "    print(\"\\n hidden at iter :\",x ,\" = \" ,hidden1)\n",
    "    x += 1\n",
    "\n",
    "inputs2 = torch.cat(inputs1).view(len(inputs1), 1, -1)\n",
    "# hidden2 = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
    "\n",
    "print(\"\\ninputs = \",inputs2)\n",
    "print(\"\\nhidden = \",hidden2)\n",
    "out2, hidden2 = lstm(inputs2,hidden2) #momken t3ml lstm(inputs2) w hwa mn gwa hy3ml el hiddens\n",
    "print(\"\\nout = \",out2)\n",
    "print(\"\\n\\n hidden = \",hidden2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(inputs2.shape) # tx=5,batch_size=1,word_embed_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dy LSTM 3dya lw d5lt klma klma el out bta3 el klma hytl3 nfs el hidden(a) bta3 el klma\n",
    "ama lw d5lt el gomla kolha el out hytl3 lel gomla kolha wel hidden hytl3 a5r hidden state bs\n",
    "fa m3na kda en el out<ty> = a<ty> dymn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n",
      "[(['The', 'dog', 'ate', 'the', 'apple'], ['DET', 'NN', 'V', 'DET', 'NN']), (['Everybody', 'read', 'that', 'book'], ['NN', 'V', 'DET', 'NN'])]\n"
     ]
    }
   ],
   "source": [
    "#model PART of speech y3ne by7dd kol klma dy noun wla verb wla DET\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "word_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'dog', 'ate', 'the', 'apple'] ['DET', 'NN', 'V', 'DET', 'NN']\n",
      "['Everybody', 'read', 'that', 'book'] ['NN', 'V', 'DET', 'NN']\n",
      "(tensor([0, 1, 2, 3, 4]), tensor([0, 1, 2, 0, 1]))\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "for x,y in training_data:\n",
    "    print(x,y)\n",
    "    dataset.append((torch.tensor([word_to_ix[word] for word in x]),torch.tensor([tag_to_ix[tag] for tag in y])))\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self,embedd_dim,hidden_dim,vocab_size,tagset_size):\n",
    "        super(LSTMTagger,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(vocab_size,embedd_dim)\n",
    "        self.lstm = nn.LSTM(embedd_dim,hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim,tagset_size)\n",
    "    def forward(self,x):\n",
    "        inputs_seq = self.embedding(x)\n",
    "        #if it was language modeling i would use _ instead of a_out\n",
    "        #but here i don't want hidden states any more hint: it use new initialied hidden states a,c\n",
    "        #each time because POS not need previous input it needs current input only\n",
    "        a_out , _ = self.lstm(inputs_seq.view(len(x), 1, -1))\n",
    "        output_tags = self.hidden2tag(a_out.view(len(x), -1))\n",
    "        output_scores = F.log_softmax(output_tags,dim = 1)\n",
    "        return output_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_ix)\n",
    "tagset_size = len(tag_to_ix)\n",
    "model = LSTMTagger(EMBEDDING_DIM,HIDDEN_DIM,vocab_size,tagset_size)\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.1)\n",
    "criterion = nn.NLLLoss()\n",
    "def train_model(model,optimizer,criterion,dataset,iter = 10):\n",
    "    for i in range(iter):\n",
    "        for x,y in dataset:\n",
    "            yhat = model(x)\n",
    "            \n",
    "            loss = criterion(yhat,y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "trained_model = train_model(model,optimizer,criterion,dataset,iter = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4])\n",
      "tensor([[-0.0471, -3.8399, -3.7075],\n",
      "        [-4.2119, -0.0466, -3.4840],\n",
      "        [-2.8518, -3.9280, -0.0806],\n",
      "        [-0.0720, -3.2803, -3.4470],\n",
      "        [-3.9195, -0.0266, -5.0474]])\n"
     ]
    }
   ],
   "source": [
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    print(inputs)\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
